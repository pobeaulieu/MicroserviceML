{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV to DataFrame\n",
    "best_community_detection_algorithm = 'Louvain' # Change this\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate service embeddings\n",
    "\n",
    "# Filter out class names not present in the embeddings dictionary\n",
    "valid_communities_df = communities_df[communities_df['class_name'].isin(class_embeddings_dict.keys())]\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = valid_communities_df.groupby('service')['class_name'].apply(\n",
    "    lambda x: sum(class_embeddings_dict[class_name] for class_name in x) / len(x)\n",
    ").to_dict()\n",
    "\n",
    "# 2. Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(embedding1, embedding2)\n",
    "        for s2, embedding2 in service_to_embedding.items() if s1 != s2\n",
    "    }\n",
    "    for s1, embedding1 in service_to_embedding.items()\n",
    "}\n",
    "\n",
    "# 3. Create dictionaries to store processed distances using merges\n",
    "merged_df = class_graph_df.merge(\n",
    "    communities_df, left_on='class1', right_on='class_name', how='inner'\n",
    ").merge(\n",
    "    communities_df, left_on='class2', right_on='class_name', how='inner', suffixes=('_1', '_2')\n",
    ")\n",
    "\n",
    "# Filter rows where services are the same and accumulate distances\n",
    "static_dict = merged_df.loc[merged_df['service_1'] != merged_df['service_2']].groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "semantic_dict = {(s1, s2): service_similarities.get(s1, {}).get(s2) for s1, s2 in static_dict.keys()}\n",
    "\n",
    "# 4. Normalize static distances\n",
    "max_static_distance = max(static_dict.values()) if static_dict else 0\n",
    "normalized_static_dict = {k: v / max_static_distance for k, v in static_dict.items()}\n",
    "\n",
    "# 5. Create the service_graph_df DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_dict[(s1, s2)], semantic_dict.get((s1, s2), 0)]\n",
    "    for s1, s2 in normalized_static_dict.keys()\n",
    "]\n",
    "\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# 6. Save service_graph_df to CSV\n",
    "service_graph_df.to_csv(f\"generated_data/graph/service/{version}_{system}_service_graph.csv\", index=False)\n",
    "\n",
    "service_graph_df.head()  # Display the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import skfuzzy as fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_weight(semantic, static, alpha=0.5, beta=0.5):\n",
    "    return alpha * static + beta * semantic\n",
    "\n",
    "# Compute edges with weights and construct the graph\n",
    "services_graph = nx.Graph([\n",
    "    (row['service1'], row['service2'], {\"weight\": edge_weight(row['semantic_distance'], row['static_distance'])})\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_distances = nx.floyd_warshall_numpy(services_graph, weight='weight')\n",
    "print(shortest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the center distances matrix into a 2D feature representation\n",
    "embedding = MDS(n_components=2, dissimilarity='precomputed')\n",
    "transformed_data = embedding.fit_transform(shortest_distances)\n",
    "\n",
    "# Determine optimal number of clusters using Elbow method\n",
    "fpcs = []\n",
    "cluster_range = range(1, 100)  # Adjust as needed\n",
    "for c_value in cluster_range:\n",
    "    _, _, _, _, _, _, fpc = fuzz.cmeans(\n",
    "        transformed_data.T, \n",
    "        c=c_value, \n",
    "        m=2, \n",
    "        error=0.005, \n",
    "        maxiter=1000\n",
    "    )\n",
    "    fpcs.append(fpc)\n",
    "\n",
    "# Plot the FPC values to determine the 'elbow'\n",
    "plt.figure()\n",
    "plt.plot(cluster_range, fpcs)\n",
    "plt.title('Fuzzy Partition Coefficient (FPC) for different cluster numbers')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('FPC')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "def detect_elbow(y_values):\n",
    "    # Get coordinates of all the points\n",
    "    n_points = len(y_values)\n",
    "    all_coords = np.vstack((range(n_points), y_values)).T\n",
    "    # Get vectors between all points from the first point to the last point\n",
    "    first_point = all_coords[0]\n",
    "    line_vector = all_coords[-1] - all_coords[0]\n",
    "    line_vector_norm = line_vector / np.sqrt(np.sum(line_vector**2))\n",
    "    \n",
    "    # Get orthogonal vectors from the first point to all points\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_prod = np.sum(vec_from_first * line_vector_norm, axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_prod, line_vector_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    \n",
    "    # Compute the distance to the line\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    \n",
    "    # Return the index of the point with max distance to the line\n",
    "    idx_elbow = np.argmax(dist_to_line)\n",
    "    return idx_elbow\n",
    "\n",
    "optimal_clusters = detect_elbow(fpcs)\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "# 4. Use transformed_center_data with scikit-fuzzy's cmeans function using the optimal cluster number\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cmeans(\n",
    "    transformed_data.T, \n",
    "    c=optimal_clusters, \n",
    "    m=2, \n",
    "    error=0.005, \n",
    "    maxiter=1000\n",
    ")\n",
    "labels = np.argmax(u, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_num in np.unique(labels):\n",
    "    cluster_points = transformed_data[labels == cluster_num]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Microservice {cluster_num + 1}')\n",
    "\n",
    "plt.title('Fuzzy C-Means Clustering')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save to microservices.txt\n",
    "with open(f\"./results/{version}_{system}_microservices.txt\", \"w\") as file:\n",
    "    for cluster_num in np.unique(labels):\n",
    "        file.write(f\"Microservice {cluster_num + 1}:\\n\")\n",
    "        cluster_services = [service_name for idx, service_name in enumerate(services_graph.nodes) if labels[idx] == cluster_num]\n",
    "        \n",
    "        for service in cluster_services:\n",
    "            file.write(f\"  - Service: {service}\\n\")\n",
    "            \n",
    "            # Retrieve and write related classes for the service\n",
    "            related_classes = communities_df[communities_df['service'] == service]['class_name'].tolist()\n",
    "            for related_class in related_classes:\n",
    "                file.write(f\"    - Class: {related_class}\\n\")\n",
    "                \n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV to DataFrame\n",
    "best_community_detection_algorithm = 'EdMot' # Change this\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate service embeddings\n",
    "\n",
    "# Filter out class names not present in the embeddings dictionary\n",
    "valid_communities_df = communities_df[communities_df['class_name'].isin(class_embeddings_dict.keys())]\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = valid_communities_df.groupby('service')['class_name'].apply(\n",
    "    lambda x: sum(class_embeddings_dict[class_name] for class_name in x) / len(x)\n",
    ").to_dict()\n",
    "\n",
    "# 2. Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(embedding1, embedding2)\n",
    "        for s2, embedding2 in service_to_embedding.items() if s1 != s2\n",
    "    }\n",
    "    for s1, embedding1 in service_to_embedding.items()\n",
    "}\n",
    "\n",
    "# 3. Create dictionaries to store processed distances using merges\n",
    "merged_df = class_graph_df.merge(\n",
    "    communities_df, left_on='class1', right_on='class_name', how='inner'\n",
    ").merge(\n",
    "    communities_df, left_on='class2', right_on='class_name', how='inner', suffixes=('_1', '_2')\n",
    ")\n",
    "\n",
    "# Filter rows where services are the same and accumulate distances\n",
    "static_dict = merged_df.loc[merged_df['service_1'] != merged_df['service_2']].groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "semantic_dict = {(s1, s2): service_similarities.get(s1, {}).get(s2) for s1, s2 in static_dict.keys()}\n",
    "\n",
    "# 4. Normalize static distances\n",
    "max_static_distance = max(static_dict.values()) if static_dict else 0\n",
    "normalized_static_dict = {k: v / max_static_distance for k, v in static_dict.items()}\n",
    "\n",
    "# 5. Create the service_graph_df DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_dict[(s1, s2)], semantic_dict.get((s1, s2), 0)]\n",
    "    for s1, s2 in normalized_static_dict.keys()\n",
    "]\n",
    "\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# 6. Save service_graph_df to CSV\n",
    "service_graph_df.to_csv(f\"generated_data/graph/service/{version}_{system}_service_graph.csv\", index=False)\n",
    "\n",
    "service_graph_df.head()  # Display the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_weight(semantic_distance, static_distance):\n",
    "    \"\"\"\n",
    "    Calculate the edge weight based on semantic and static distances.\n",
    "    \"\"\"\n",
    "    semantic_factor = 1 / semantic_distance if semantic_distance != 0 else 0\n",
    "    return 0.1 * static_distance + 100 * semantic_factor\n",
    "\n",
    "\n",
    "def build_adjacency_matrix(graph, df):\n",
    "    \"\"\"\n",
    "    Generate a directed adjacency matrix from the service graph and the data frame.\n",
    "    \"\"\"\n",
    "    nodes = list(graph.nodes)\n",
    "    node_to_index = {node: idx for idx, node in enumerate(nodes)}\n",
    "    matrix = np.full((len(nodes), len(nodes)), np.inf)\n",
    "    src_indices = df['service1'].map(node_to_index).values\n",
    "    dest_indices = df['service2'].map(node_to_index).values\n",
    "    static_dists = df['static_distance'].replace(0, np.inf).values\n",
    "    matrix[src_indices, dest_indices] = 100 / static_dists\n",
    "    return matrix, nodes\n",
    "\n",
    "\n",
    "def compute_shortest_paths(matrix):\n",
    "    \"\"\"\n",
    "    Determine shortest paths for all node pairs using the Floyd Warshall algorithm.\n",
    "    \"\"\"\n",
    "    distances = matrix.copy()\n",
    "    num_nodes = distances.shape[0]\n",
    "    for k in range(num_nodes):\n",
    "        distances = np.minimum(distances, distances[:, k][:, np.newaxis] + distances[np.newaxis, :, k])\n",
    "    return distances\n",
    "\n",
    "\n",
    "def incorporate_semantic_distances(shortest_distances, nodes, df):\n",
    "    \"\"\"\n",
    "    Integrate semantic distances into the shortest distances matrix.\n",
    "    \"\"\"\n",
    "    for i, service_i in enumerate(nodes):\n",
    "        for j, service_j in enumerate(nodes):\n",
    "            distance_series = df.query(f'service1 == \"{service_i}\" & service2 == \"{service_j}\"')['semantic_distance']\n",
    "            distance = distance_series.iloc[0] if not distance_series.empty else np.nan\n",
    "            shortest_distances[i][j] += (distance * 2 if not np.isnan(distance) else 0)\n",
    "    return shortest_distances\n",
    "\n",
    "\n",
    "def fuzzy_weight(service_idx, center_idx, distances, center_indices):\n",
    "    \"\"\"\n",
    "    Compute the fuzzy weight of a service relative to a center node.\n",
    "    \"\"\"\n",
    "    m, coef = 3, 2 / (3 - 1)\n",
    "    direct_distance = distances[service_idx][center_idx]\n",
    "    \n",
    "    if direct_distance == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    distance_ratio_sum = sum(\n",
    "        direct_distance / (distances[service_idx][idx] if distances[service_idx][idx] != 0 else float('inf'))\n",
    "        for idx in center_indices\n",
    "    )\n",
    "\n",
    "    return 1 / math.pow(distance_ratio_sum, coef)\n",
    "\n",
    "\n",
    "def map_centers_to_services(graph, distances, center_prefix=\"Application\"):\n",
    "    \"\"\"\n",
    "    Associate center nodes to related services based on the fuzzy weights.\n",
    "    \"\"\"\n",
    "    centers = {name: idx for idx, name in enumerate(graph.nodes) if name.startswith(center_prefix)}\n",
    "    mapping = {}\n",
    "\n",
    "    for center_name, center_idx in centers.items():\n",
    "        related = [\n",
    "            service_name \n",
    "            for service_idx, service_name in enumerate(graph.nodes) \n",
    "            if service_name != center_name and fuzzy_weight(service_idx, center_idx, distances, list(centers.values())) * 100 > 9\n",
    "        ]\n",
    "        mapping[center_name] = related\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def save_center_service_mapping_to_file(center_service_mapping, communities_df, filename=\"./results/microservices.txt\"):\n",
    "    \"\"\"\n",
    "    Save the center_service_mapping to a file, mapping back services to their corresponding classes.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for center_name, services in center_service_mapping.items():\n",
    "            file.write(f\"Center Node: {center_name}\\n\")\n",
    "            \n",
    "            for service in services:\n",
    "                file.write(f\"  - Service: {service}\\n\")\n",
    "                \n",
    "                # Retrieve and write related classes for the service\n",
    "                related_classes = communities_df[communities_df['service'] == service]['class_name'].tolist()\n",
    "                for related_class in related_classes:\n",
    "                    file.write(f\"    - Class: {related_class}\\n\")\n",
    "                    \n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute edges with weights and construct the graph\n",
    "services_graph = nx.Graph([\n",
    "    (row['service1'], row['service2'], {\"weight\": compute_edge_weight(row['semantic_distance'], row['static_distance'])})\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])\n",
    "\n",
    "adjacency_matrix, node_names = build_adjacency_matrix(services_graph, service_graph_df)\n",
    "shortest_paths = compute_shortest_paths(adjacency_matrix)\n",
    "shortest_paths = incorporate_semantic_distances(shortest_paths, node_names, service_graph_df)\n",
    "center_service_mapping = map_centers_to_services(services_graph, shortest_paths)\n",
    "save_center_service_mapping_to_file(center_service_mapping, communities_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

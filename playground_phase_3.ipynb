{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV to DataFrame\n",
    "best_community_detection_algorithm = 'Louvain' # Change this\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate service embeddings\n",
    "\n",
    "# Filter out class names not present in the embeddings dictionary\n",
    "valid_communities_df = communities_df[communities_df['class_name'].isin(class_embeddings_dict.keys())]\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = valid_communities_df.groupby('service')['class_name'].apply(\n",
    "    lambda x: sum(class_embeddings_dict[class_name] for class_name in x) / len(x)\n",
    ").to_dict()\n",
    "\n",
    "# 2. Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(embedding1, embedding2)\n",
    "        for s2, embedding2 in service_to_embedding.items() if s1 != s2\n",
    "    }\n",
    "    for s1, embedding1 in service_to_embedding.items()\n",
    "}\n",
    "\n",
    "# 3. Create dictionaries to store processed distances using merges\n",
    "merged_df = class_graph_df.merge(\n",
    "    communities_df, left_on='class1', right_on='class_name', how='inner'\n",
    ").merge(\n",
    "    communities_df, left_on='class2', right_on='class_name', how='inner', suffixes=('_1', '_2')\n",
    ")\n",
    "\n",
    "# Filter rows where services are the same and accumulate distances\n",
    "static_dict = merged_df.loc[merged_df['service_1'] != merged_df['service_2']].groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "semantic_dict = {(s1, s2): service_similarities.get(s1, {}).get(s2) for s1, s2 in static_dict.keys()}\n",
    "\n",
    "# 4. Normalize static distances\n",
    "max_static_distance = max(static_dict.values()) if static_dict else 0\n",
    "normalized_static_dict = {k: v / max_static_distance for k, v in static_dict.items()}\n",
    "\n",
    "# 5. Create the service_graph_df DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_dict[(s1, s2)], semantic_dict.get((s1, s2), 0)]\n",
    "    for s1, s2 in normalized_static_dict.keys()\n",
    "]\n",
    "\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# 6. Save service_graph_df to CSV\n",
    "service_graph_df.to_csv(f\"generated_data/graph/service/{version}_{system}_service_graph.csv\", index=False)\n",
    "\n",
    "service_graph_df.head()  # Display the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_weight(semantic, static):\n",
    "    # return 0.1 * static + 100 * (1 / semantic if semantic else 0) # why, static distance has almost no influence (ex. weight = 0.03 (static) + 200 (semantic) = 200.03)\n",
    "    return static + semantic / 2 # both static and semantic will contribute the same\n",
    "\n",
    "# Compute edges with weights and construct the graph\n",
    "services_graph = nx.Graph([\n",
    "    (row['service1'], row['service2'], {\"weight\": edge_weight(row['semantic_distance'], row['static_distance'])})\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "def create_adjacency_matrix(graph, df):\n",
    "    \"\"\"Create a directed adjacency matrix from service graph and dataframe.\"\"\"\n",
    "    nodes = list(graph.nodes)\n",
    "    node_to_index = {node: idx for idx, node in enumerate(nodes)}\n",
    "    \n",
    "    # Initialize the matrix with inf values\n",
    "    matrix = np.full((len(nodes), len(nodes)), np.inf)\n",
    "    \n",
    "    # Extract the source, destination and static distance columns\n",
    "    src_indices = df['service1'].map(node_to_index).values\n",
    "    dest_indices = df['service2'].map(node_to_index).values\n",
    "    static_dists = df['static_distance'].replace(0, np.inf).values\n",
    "    \n",
    "    # Use the indices for efficient assignment\n",
    "    matrix[src_indices, dest_indices] = 100 / static_dists # why\n",
    "\n",
    "    return matrix, nodes\n",
    "\n",
    "# Alternative: Create adjacency matrix using networkx's adjacency_matrix function\n",
    "# adj_matrix = nx.adjacency_matrix(services_graph, weight='weight').toarray()\n",
    "\n",
    "# Floyd Warshall Algorithm\n",
    "def floyd_warshall(adj_matrix):\n",
    "    \"\"\"Compute shortest paths for all pairs of nodes using the Floyd Warshall algorithm (optimized with numpy).\n",
    "    \n",
    "    Parameters:\n",
    "    - adj_matrix (numpy.array): The adjacency matrix of the graph.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.array: Matrix of shortest path distances.\n",
    "    \"\"\"\n",
    "    distance = adj_matrix.copy()\n",
    "    num_vertices = distance.shape[0]\n",
    "    \n",
    "    for k in range(num_vertices):\n",
    "        distance = np.minimum(distance, distance[:, k][:, np.newaxis] + distance[np.newaxis, :, k])\n",
    "                \n",
    "    return distance\n",
    "\n",
    "# Alternative: Use networkx's floyd_warshall_numpy function for shortest paths computation\n",
    "# shortest_distances = nx.floyd_warshall_numpy(services_graph, weight='weight') # skips the need for adjacency matrix\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix, nodes_list = create_adjacency_matrix(services_graph, service_graph_df)\n",
    "print(adj_matrix)\n",
    "\n",
    "# Calculate shortest paths using Floyd Warshall\n",
    "shortest_distances = floyd_warshall(adj_matrix)\n",
    "print(shortest_distances)\n",
    "\n",
    "for i, service_i in enumerate(nodes_list):\n",
    "    for j, service_j in enumerate(nodes_list):\n",
    "        distance = service_graph_df[\n",
    "            (service_graph_df['service1'] == service_i) & \n",
    "            (service_graph_df['service2'] == service_j)\n",
    "        ]['semantic_distance'].iloc[0] if not service_graph_df[\n",
    "            (service_graph_df['service1'] == service_i) & \n",
    "            (service_graph_df['service2'] == service_j)\n",
    "        ]['semantic_distance'].empty else 0\n",
    "        \n",
    "        shortest_distances[i][j] += distance * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tout le bloc du haut pourrait être remplacé par ceci:\n",
    "shortest_distances = nx.floyd_warshall_numpy(services_graph, weight='weight')\n",
    "print(shortest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize services by their application and index them\n",
    "application_indices = {node: index for index, node in enumerate(nodes_list) if node.startswith(\"Application\")}\n",
    "\n",
    "# Calculate fuzzy weight for a service in relation to an application\n",
    "def fuzzy_weight(service_index, app_index, distances):\n",
    "    m = 3\n",
    "    coef = 2 / (m - 1)\n",
    "    \n",
    "    distance_to_app = distances[service_index][app_index]\n",
    "    weight_sum = sum(distance_to_app / distances[service_index][idx] for idx in application_indices.values())\n",
    "    \n",
    "    return 1 / math.pow(weight_sum, coef)\n",
    "\n",
    "# Compile the results into a dictionary\n",
    "fuzzy_service_mapping = {}\n",
    "\n",
    "for app_name, app_index in application_indices.items():\n",
    "    related_services = []\n",
    "\n",
    "    for service_index, service_name in enumerate(nodes_list):\n",
    "        if service_name != app_name and fuzzy_weight(service_index, app_index, shortest_distances) * 100 > 9:\n",
    "            related_services.append(service_name)\n",
    "\n",
    "    fuzzy_service_mapping[app_name] = related_services\n",
    "\n",
    "# Display the first few items in the fuzzy service mapping for verification\n",
    "dict(list(fuzzy_service_mapping.items())[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_team\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "phase1_model = 'codebert' \n",
    "phase2_model = 'GirvanNewman' # All options: Louvain, EdMot, Infomap, LabelPropagation, FastGreedy, GirvanNewman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import save_microservices_to_txt, save_microservices_to_csv\n",
    "import networkx as nx\n",
    "import skfuzzy as fuzz\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from graphs import remove_zero_weight_edges, print_graph, construct_dissimilarity_matrix\n",
    "from optimization import detect_elbow\n",
    "from cluster_analysis import assign_clusters_based_on_comparative_ratios, merge_overlapping_clusters, identify_standalone_services\n",
    "from normalization import normalize_data, normalize_memberships\n",
    "from distances import compute_static_distances_for_service_pairs, compute_semantic_distances_for_service_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for filename\n",
    "filename = f\"generated_data/graph/service/{version}_{system}_service_graph.csv\"\n",
    "\n",
    "# Load the data from CSV files\n",
    "communities_df = pd.read_csv(f\"generated_data/phase2_service_clustering/{phase2_model}/{version}_{system}_{phase1_model}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "embeddings_df = pd.read_csv(f\"generated_data/embedding/{version}_{system}_{phase1_model}_embeddings.csv\")\n",
    "\n",
    "# Extract class names and their embeddings from the embeddings DataFrame\n",
    "class_names = embeddings_df.iloc[:, 0].str.split(';', expand=True)[0]\n",
    "embeddings = embeddings_df.iloc[:, 1:].values\n",
    "class_embeddings_dict = dict(zip(class_names, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute static and semantic distances between service pairs\n",
    "static_distances = compute_static_distances_for_service_pairs(class_graph_df, communities_df)\n",
    "semantic_distances = compute_semantic_distances_for_service_pairs(class_embeddings_dict, communities_df)\n",
    "\n",
    "# Normalize the distances between 0 and 1\n",
    "normalized_static_distances = normalize_data(static_distances)\n",
    "normalized_semantic_distances = normalize_data(semantic_distances)\n",
    "\n",
    "# Create the service graph DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_distances.get((s1, s2), 0), normalized_semantic_distances.get((s1, s2), 0)]\n",
    "    for s1, s2 in static_distances.keys()\n",
    "]\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# Save the service graph DataFrame to a CSV file\n",
    "service_graph_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_weight(semantic, static, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Compute the edge weight based on semantic and static similarity.\n",
    "\n",
    "    Parameters:\n",
    "        semantic (float): The semantic similarity value between two services.\n",
    "        static (float): The static similarity value between two services.\n",
    "        alpha (float): The weight given to the static similarity.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed edge weight.\n",
    "    \"\"\"\n",
    "    if not (0 <= semantic <= 1) or not (0 <= static <= 1):\n",
    "        raise ValueError(\"Both 'semantic' and 'static' values should be between 0 and 1.\")\n",
    "    \n",
    "    beta = 1 - alpha\n",
    "    return alpha * static + beta * semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_graph_df = pd.read_csv(filename)\n",
    "\n",
    "services_graph = nx.DiGraph([\n",
    "    (row['service1'], row['service2'], {\"weight\": compute_edge_weight(row['semantic_distance'], row['static_distance'], alpha=1.0)}) # adjust alpha here\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])\n",
    "\n",
    "remove_zero_weight_edges(services_graph)\n",
    "print_graph(services_graph, \"weight\")\n",
    "nodes_list = list(services_graph.nodes)\n",
    "\n",
    "# Construct the dissimilarity matrix\n",
    "dissimilarity_matrix = construct_dissimilarity_matrix(services_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Fuzzy C-means from Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FUZZINESS = 2\n",
    "ERROR_THRESHOLD = 0.005\n",
    "MAX_ITERATIONS = 1000\n",
    "CLUSTER_RANGE = range(2, 100)\n",
    "\n",
    "def determine_optimal_clusters(data):\n",
    "    \"\"\"Determines the optimal number of clusters using the Elbow method.\"\"\"\n",
    "    fpc_values = []\n",
    "    for c_value in CLUSTER_RANGE:\n",
    "        _, _, _, _, _, _, fpc = fuzz.cmeans(\n",
    "            data.T, \n",
    "            c=c_value, \n",
    "            m=FUZZINESS, \n",
    "            error=ERROR_THRESHOLD, \n",
    "            maxiter=MAX_ITERATIONS\n",
    "        )\n",
    "        fpc_values.append(fpc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(CLUSTER_RANGE, fpc_values)\n",
    "    plt.title('Fuzzy Partition Coefficient (FPC) for different cluster numbers')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('FPC')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return detect_elbow(fpc_values)\n",
    "\n",
    "def fuzzy_cmeans_clustering(data, node_labels, optimal_clusters):\n",
    "    \"\"\"\n",
    "    Clusters the data using Fuzzy C-Means, returning only significant memberships.\n",
    "    \n",
    "    Parameters:\n",
    "        data: 2D array of shape (features, samples), the input data.\n",
    "        node_labels: List of labels corresponding to the samples.\n",
    "        optimal_clusters: The number of clusters to form as well as the number of centroids to generate.\n",
    "        membership_threshold: Threshold for membership values to determine significant cluster membership.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with node labels as keys and lists of (cluster, membership) tuples as values.\n",
    "    \"\"\"\n",
    "    cntr, u, _, _, _, _, _ = fuzz.cmeans(\n",
    "        data.T,\n",
    "        c=optimal_clusters,\n",
    "        m=FUZZINESS,\n",
    "        error=ERROR_THRESHOLD,\n",
    "        maxiter=MAX_ITERATIONS\n",
    "    )\n",
    "    \n",
    "    # Create a dictionary to store memberships for each node\n",
    "    memberships = {label: [] for label in node_labels}\n",
    "    for i, label in enumerate(node_labels):\n",
    "        memberships[label] = [(f'cluster{j+1}', u[j, i]) for j in range(optimal_clusters)]\n",
    "\n",
    "    return memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clusters = determine_optimal_clusters(dissimilarity_matrix)\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships = fuzzy_cmeans_clustering(dissimilarity_matrix, nodes_list, optimal_clusters)\n",
    "normalized_memberships = normalize_memberships(memberships)\n",
    "fuzzy_memberships = assign_clusters_based_on_comparative_ratios(normalized_memberships)\n",
    "\n",
    "for node, memberships in fuzzy_memberships.items():\n",
    "        print(f\"{node}: {', '.join(f'{cluster} ({membership:.2f})' for cluster, membership in memberships)}\")\n",
    "\n",
    "save_microservices_to_txt(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/cmeans/{version}_{system}_{phase2_model}_microservices.txt\")\n",
    "save_microservices_to_csv(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/cmeans/{version}_{system}_{phase2_model}_microservices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Custom fuzzy C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFuzzyCMeans:\n",
    "    \"\"\"\n",
    "    A custom implementation of the Fuzzy C-Means clustering algorithm.\n",
    "\n",
    "    Attributes:\n",
    "        center_type_indices: A dict mapping the service type to its index.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.center_type = 'Application'\n",
    "        self.center_type_indices = {}\n",
    "\n",
    "    def calculate_membership(self, service_idx, center_idx, distances):\n",
    "        \"\"\"\n",
    "        Calculate the fuzzy membership of a service to a cluster center.\n",
    "        \n",
    "        Parameters:\n",
    "        service_idx (int): Index of the service for which membership is being calculated.\n",
    "        center_idx (int): Index of the cluster center to which membership is being calculated.\n",
    "        distances (list): Matrix of distances between services and cluster centers.\n",
    "        \n",
    "        Returns:\n",
    "        float: The membership value of the service to the cluster center.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10  # Small value to avoid division by zero.\n",
    "        distance_to_center = distances[service_idx][center_idx]\n",
    "\n",
    "        membership_sum = 0.0\n",
    "\n",
    "        if distance_to_center == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        for other_center_idx in self.center_type_indices.values():\n",
    "            if other_center_idx != service_idx:\n",
    "                # Direct distances to other centers\n",
    "                other_distance_to_center = distances[service_idx][other_center_idx]\n",
    "\n",
    "                membership_ratio = distance_to_center / (other_distance_to_center + epsilon)\n",
    "                membership_sum += membership_ratio ** 2\n",
    "\n",
    "        # The membership value is the inverse of the sum, normalized by the number of centers\n",
    "        return 1 / membership_sum if membership_sum > 0 else 0\n",
    "\n",
    "        \n",
    "    def cluster_services(self, services_list, distances):\n",
    "        \"\"\"\n",
    "        Cluster services based on their membership values to each cluster center.\n",
    "\n",
    "        Parameters:\n",
    "        services_list (list): List of all services to be clustered.\n",
    "        distances (list): Matrix of distances between services and cluster centers.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with services as keys and lists of (cluster, membership) tuples as values.\n",
    "        \"\"\"\n",
    "\n",
    "        self.center_type_indices = {\n",
    "            service: idx for idx, service in enumerate(nodes_list) if service.startswith(self.center_type)\n",
    "        }\n",
    "        \n",
    "        # Calculate raw membership values for each service to each cluster center\n",
    "        memberships = {}\n",
    "        for service_idx, service in enumerate(services_list):\n",
    "            service_memberships = []\n",
    "            for idx, (_, center_idx) in enumerate(self.center_type_indices.items()):\n",
    "                membership_value = self.calculate_membership(service_idx, center_idx, distances)\n",
    "                service_memberships.append((f\"cluster{idx+1}\", membership_value))\n",
    "            memberships[service] = service_memberships\n",
    "\n",
    "        return memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships = CustomFuzzyCMeans().cluster_services(nodes_list, dissimilarity_matrix)\n",
    "normalized_memberships = normalize_memberships(memberships)\n",
    "fuzzy_memberships = assign_clusters_based_on_comparative_ratios(normalized_memberships)\n",
    "\n",
    "# Call the method to merge clusters with overlap above merge_threshold\n",
    "fuzzy_memberships = identify_standalone_services(fuzzy_memberships)\n",
    "fuzzy_memberships = merge_overlapping_clusters(fuzzy_memberships, overlap_threshold=0.3)\n",
    "\n",
    "for node, memberships in fuzzy_memberships.items():\n",
    "        print(f\"{node}: {', '.join(f'{cluster} ({membership:.2f})' for cluster, membership in memberships)}\")\n",
    "        \n",
    "save_microservices_to_txt(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/custom_cmeans/{version}_{system}_{phase2_model}_microservices.txt\")\n",
    "save_microservices_to_csv(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/custom_cmeans/{version}_{system}_{phase2_model}_microservices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(dissimilarity_matrix, max_d):\n",
    "    Z = linkage(squareform(dissimilarity_matrix), method='average')\n",
    "    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "    return clusters\n",
    "\n",
    "def calculate_wcss(clusters, dissimilarity_matrix):\n",
    "    wcss = 0\n",
    "    for i in np.unique(clusters):\n",
    "        cluster_points = dissimilarity_matrix[clusters == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        wcss += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wcss\n",
    "\n",
    "def hierarchical_clustering(dissimilarity_matrix, nodes, max_d):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering and return fuzzy memberships.\n",
    "\n",
    "    Parameters:\n",
    "    - dissimilarity_matrix: The matrix of dissimilarity between nodes.\n",
    "    - nodes: The list of node labels.\n",
    "    - max_d: The threshold to cut the dendrogram to form clusters.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with node labels as keys and a list of (cluster, membership) tuples as values.\n",
    "    \"\"\"\n",
    "    # Perform the hierarchical clustering\n",
    "    clusters = perform_clustering(dissimilarity_matrix, max_d)\n",
    "\n",
    "    # Calculate the centroids of each cluster\n",
    "    centroids = {i: np.mean(dissimilarity_matrix[clusters == i], axis=0) for i in np.unique(clusters)}\n",
    "\n",
    "    # Initialize the fuzzy memberships dictionary\n",
    "    memberships = {}\n",
    "\n",
    "    for node_idx, node in enumerate(nodes):\n",
    "        # Calculate distances to centroids\n",
    "        distances = {f'cluster{i}': np.linalg.norm(centroids[i] - dissimilarity_matrix[node_idx]) for i in centroids}\n",
    "        \n",
    "        # Invert the distances to get membership strengths (higher distance -> lower membership)\n",
    "        inverted_memberships = {cluster_label: 1 / (distance + 1e-5) for cluster_label, distance in distances.items()}\n",
    "        \n",
    "        # Add the inverted memberships to the raw_memberships dictionary for the node\n",
    "        memberships[node] = list(inverted_memberships.items())\n",
    "    \n",
    "    return memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the elbow\n",
    "wcss_list = []\n",
    "max_d_values = np.linspace(0.1, 1.0, 10)  # Adjust the range and step as needed\n",
    "\n",
    "for max_d in max_d_values:\n",
    "    clusters = perform_clustering(dissimilarity_matrix, max_d)\n",
    "    wcss = calculate_wcss(clusters, dissimilarity_matrix)\n",
    "    wcss_list.append(wcss)\n",
    "\n",
    "elbow_index = detect_elbow(wcss_list)\n",
    "optimal_max_d = max_d_values[elbow_index]\n",
    "\n",
    "print(f\"Optimal max_d is: {optimal_max_d}\")\n",
    "\n",
    "# Plotting the results with the elbow point highlighted\n",
    "plt.plot(max_d_values, wcss_list, 'bo-')\n",
    "plt.plot(max_d_values[elbow_index], wcss_list[elbow_index], 'ro')\n",
    "plt.title('Elbow Method For Optimal max_d')\n",
    "plt.xlabel('max_d')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships = hierarchical_clustering(dissimilarity_matrix, nodes_list, max_d=optimal_max_d)\n",
    "\n",
    "# Normalize the inverted memberships\n",
    "normalized_memberships = normalize_memberships(memberships)\n",
    "    \n",
    "# Assign services to clusters based on peaks in their membership distributions\n",
    "fuzzy_memberships = assign_clusters_based_on_comparative_ratios(normalized_memberships)\n",
    "\n",
    "for node, memberships in fuzzy_memberships.items():\n",
    "        print(f\"{node}: {', '.join(f'{cluster} ({membership:.2f})' for cluster, membership in memberships)}\")\n",
    "\n",
    "save_microservices_to_txt(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/hierarchical/{version}_{system}_{phase2_model}_microservices.txt\")\n",
    "\n",
    "save_microservices_to_csv(fuzzy_memberships, communities_df, \n",
    "                           f\"generated_data/phase3_microservice_clustering/hierarchical/{version}_{system}_{phase2_model}_microservices.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Measures (F-Measure, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_helpers import generate_microservices_clustering_results\n",
    "\n",
    "models = ['custom_cmeans', 'cmeans', 'hierarchical']\n",
    "\n",
    "matching_threshold = 0.8\n",
    "\n",
    "generate_microservices_clustering_results(models, phase2_model, phase1_model, version, system, matching_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

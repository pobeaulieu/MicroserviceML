{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV to DataFrame\n",
    "best_community_detection_algorithm = 'EdMot' # Change this\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate service embedding\n",
    "def calculate_service_embedding(service_classes):\n",
    "    return sum(class_embeddings_dict[class_name] for class_name in service_classes) / len(service_classes)\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = communities_df.groupby('service')['class_name'].apply(lambda x: calculate_service_embedding(x)).to_dict()\n",
    "\n",
    "# Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(service_to_embedding[s1], service_to_embedding[s2])\n",
    "        for s2 in service_to_embedding if s1 != s2\n",
    "    }\n",
    "    for s1 in service_to_embedding\n",
    "}\n",
    "\n",
    "# Create dictionaries to store processed distances\n",
    "semantic_dict = {}\n",
    "static_dict = {}\n",
    "for row in class_graph_df.itertuples(index=False):\n",
    "    mrs1 = communities_df.loc[communities_df['class_name'] == row.class1]\n",
    "    mrs2 = communities_df.loc[communities_df['class_name'] == row.class2]\n",
    "\n",
    "    if mrs1.empty or mrs2.empty or mrs1['service'].values[0] == mrs2['service'].values[0]:\n",
    "       continue\n",
    "    else:\n",
    "        service1 = mrs1['service'].values[0]\n",
    "        service2 = mrs2['service'].values[0]\n",
    "\n",
    "    static_dict[(service1, service2)] = float(static_dict.get((service1, service2), 0)) + row.static_distance\n",
    "    semantic_dict[(service1, service2)] = service_similarities.get(service1, {}).get(service2)\n",
    "\n",
    "\n",
    "# Rest of your code remains the same\n",
    "service_graph_data = []\n",
    "\n",
    "for s1, s2 in static_dict.keys():\n",
    "\n",
    "    semantic_distance = semantic_dict.get((s1, s2), 0)\n",
    "    static_distance = static_dict.get((s1, s2), 0)\n",
    "    service_graph_data.append([s1, s2, static_distance, semantic_distance])\n",
    "\n",
    "\n",
    "# Create service_graph_df DataFrame\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "# Output service_graph_df\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Save service_graph_df to CSV (we may need to add the community detection algo in the filename)\n",
    "service_graph_df.to_csv(f\"generated_data/graph/service/{version}_{system}_service_graph.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a graph and add edges\n",
    "GServices = nx.Graph()\n",
    "for d in service_graph_data:\n",
    "    GServices.add_edge(d[0], d[1], weight=0.5 * d[2] +0.5 * d[3])\n",
    "\n",
    "node_list = list(GServices.nodes())\n",
    "\n",
    "# Create an empty adjacency matrix filled with zeros\n",
    "num_nodes = len(node_list)\n",
    "adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "# Populate the adjacency matrix with edge weights\n",
    "for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "        if i == j:\n",
    "            # Diagonal elements (self-loops) can be set to zero or any other appropriate value\n",
    "            adjacency_matrix[i][j] = 0\n",
    "        else:\n",
    "            # Check if there is an edge between the nodes\n",
    "            if GServices.has_edge(node_list[i], node_list[j]):\n",
    "                # Get the weight of the edge\n",
    "                edge_data = GServices[node_list[i]][node_list[j]]\n",
    "                edge_weight = edge_data['weight']\n",
    "                adjacency_matrix[i][j] = edge_weight\n",
    "\n",
    "# Create a DataFrame for the adjacency matrix with service names\n",
    "adjacency_df = pd.DataFrame(adjacency_matrix, index=node_list, columns=node_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = f\"generated_data/graph/service/{version}_{system}_adjacency_matrix.csv\"\n",
    "adjacency_df.to_csv(csv_filename)\n",
    "\n",
    "print(f\"Adjacency matrix saved to {csv_filename}\")\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Define a range of cluster numbers to consider\n",
    "num_clusters_range = range(1, 20)  # Adjust the range as needed\n",
    "\n",
    "# Initialize empty lists to store the results\n",
    "fcm_scores = []\n",
    "fpc_scores = []\n",
    "\n",
    "# Calculate Fuzzy C-Means scores and FPC scores for different cluster numbers\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Specify the fuzziness coefficient (e.g., m=2.0)\n",
    "    m = 3.0\n",
    "    fcm = fuzz.cmeans(adjacency_matrix, num_clusters, m, error=0.005, maxiter=1000)\n",
    "    fcm_scores.append(fcm[3])  # Appending the mean squared error (MSE) to scores\n",
    "\n",
    "    # Calculate FPC score using the formula FPC = (Tr(B) / W) / (1 - Tr(W) / W)\n",
    "    B = np.linalg.norm(fcm[0], axis=0)\n",
    "    W = np.mean(fcm[3])\n",
    "    fpc = (np.sum(B) / W) / (1 - W)\n",
    "    fpc_scores.append(fpc)\n",
    "\n",
    "print(\"fpc_score for each number of MS\")\n",
    "print(fpc_scores)\n",
    "# Find the number of clusters with the highest FPC score\n",
    "\n",
    "\n",
    "optimal_num_clusters = num_clusters_range[np.argmax(fpc_scores)]\n",
    "\n",
    "print(\"Optimal number of MS\")\n",
    "print(optimal_num_clusters)\n",
    "\n",
    "# Apply Fuzzy C-Means clustering with the optimal number of clusters\n",
    "fcm = fuzz.cmeans(adjacency_matrix, optimal_num_clusters, m, error=0.005, maxiter=1000)\n",
    "\n",
    "# Get cluster memberships for each Application Service\n",
    "membership_degrees = fcm[0]\n",
    "\n",
    "# Calculate the threshold dynamically based on the mean membership degree\n",
    "mean_membership_degrees = np.mean(membership_degrees, axis=1)\n",
    "threshold = np.mean(mean_membership_degrees)\n",
    "print(mean_membership_degrees)\n",
    "print(\"threshold\" + str(threshold))\n",
    "# Create a dictionary to map Application Services to their clusters\n",
    "service_clusters = {}\n",
    "for i, service in enumerate(GServices.nodes()):\n",
    "    for cluster in range(optimal_num_clusters):\n",
    "        if membership_degrees[cluster][i] > threshold:\n",
    "            print(membership_degrees[cluster])\n",
    "            if cluster not in service_clusters:\n",
    "                service_clusters[cluster] = []\n",
    "            service_clusters[cluster].append(service)\n",
    "\n",
    "output_file_path = f\"generated_data/graph/service/{version}_{system}_microservices.txt\"\n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for ms, s in service_clusters.items():\n",
    "        file.write(f\"Microservice {ms + 1}:\\n\")\n",
    "        for service in s:\n",
    "            file.write(f\"  - {service}\\n\")\n",
    "\n",
    "print(\"Microservices assignments saved to\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

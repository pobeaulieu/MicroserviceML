{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"codebert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert\n",
    "best_community_detection_algorithm = 'Louvain' # All options: Louvain, EdMot, Infomap, LabelPropagation, FastGreedy, GirvanNewman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from utils import save_microservices_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "embeddings_df = pd.read_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Extract class names and their embeddings\n",
    "class_names = embeddings_df.iloc[:, 0].str.split(';', expand=True)[0]\n",
    "embeddings = embeddings_df.iloc[:, 1:].values\n",
    "class_embeddings_dict = dict(zip(class_names, embeddings))\n",
    "\n",
    "def compute_static_distances_for_service_pairs(class_graph, communities):\n",
    "    merged_df = class_graph.merge(communities, left_on='class1', right_on='class_name').merge(\n",
    "        communities, left_on='class2', right_on='class_name', suffixes=('_1', '_2')\n",
    "    )\n",
    "    inter_service_df = merged_df[merged_df['service_1'] != merged_df['service_2']]\n",
    "    return inter_service_df.groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "\n",
    "def compute_service_embeddings(embeddings_dict, communities):\n",
    "    \"\"\"Compute service embeddings by averaging class embeddings for each service, skipping missing embeddings.\"\"\"\n",
    "    service_embeddings = {}\n",
    "    for service, class_group in communities.groupby('service')['class_name']:\n",
    "        class_embeddings = [embeddings_dict[class_name] for class_name in class_group if class_name in embeddings_dict]\n",
    "        if class_embeddings:\n",
    "            service_embeddings[service] = np.mean(class_embeddings, axis=0)\n",
    "    return service_embeddings\n",
    "\n",
    "def compute_semantic_distances_for_service_pairs(embeddings_dict, communities):\n",
    "    service_embeddings = compute_service_embeddings(embeddings_dict, communities)\n",
    "    \n",
    "    services = list(service_embeddings.keys())\n",
    "    semantic_distances = {}\n",
    "    for i, s1 in enumerate(services):\n",
    "        for j, s2 in enumerate(services):\n",
    "            if i != j:\n",
    "                distance = 1 - spatial.distance.cosine(service_embeddings[s1], service_embeddings[s2])\n",
    "                semantic_distances[(s1, s2)] = distance\n",
    "    \n",
    "    return semantic_distances\n",
    "\n",
    "def normalize_data(data):\n",
    "    min_val, max_val = min(data.values()), max(data.values())\n",
    "    range_val = max_val - min_val\n",
    "    return {k: (v - min_val) / range_val for k, v in data.items()} if range_val else {k: 0 for k, v in data.items()}\n",
    "\n",
    "# Compute static and semantic distances\n",
    "static_distances = compute_static_distances_for_service_pairs(class_graph_df, communities_df)\n",
    "semantic_distances = compute_semantic_distances_for_service_pairs(class_embeddings_dict, communities_df)\n",
    "\n",
    "# Normalize the distances\n",
    "normalized_static_distances = normalize_data(static_distances)\n",
    "normalized_semantic_distances = normalize_data(semantic_distances)\n",
    "\n",
    "# Create the service graph DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_distances.get((s1, s2), 0), normalized_semantic_distances.get((s1, s2), 0)]\n",
    "    for s1, s2 in static_distances.keys()\n",
    "]\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# Save the DataFrame\n",
    "filename = f\"generated_data/graph/service/{version}_{system}_service_graph.csv\"\n",
    "service_graph_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import skfuzzy as fuzz\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_weight(semantic, static, alpha=0.5):\n",
    "    if not (0 <= semantic <= 1) or not (0 <= static <= 1):\n",
    "        raise ValueError(\"Both 'semantic' and 'static' values should be between 0 and 1.\")\n",
    "    \n",
    "    beta = 1 - alpha\n",
    "    return alpha * static + beta * semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "services_graph = nx.Graph([\n",
    "    (row['service1'], row['service2'], {\"weight\": compute_edge_weight(row['semantic_distance'], row['static_distance'])}) # adjust alpha (weight of static distance)\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])\n",
    "\n",
    "# Remove edges with weight 0\n",
    "services_graph.remove_edges_from([(u, v) for u, v, weight in services_graph.edges(data='weight') if weight == 0])\n",
    "\n",
    "# Print pairs of nodes and their edge weights\n",
    "print(\"Edge weights:\")\n",
    "for u, v, weight in services_graph.edges(data='weight'):\n",
    "    print(f\"{u} - {v}: {weight}\")\n",
    "\n",
    "shortest_distances = nx.floyd_warshall_numpy(services_graph, weight='weight')\n",
    "\n",
    "# Replace 'inf' with a large value, such as 1e9\n",
    "shortest_distances[shortest_distances == np.inf] = 1e9\n",
    "\n",
    "print(\"Shortest distances matrix:\")\n",
    "print(shortest_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Fuzzy C-means from Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (change if needed)\n",
    "N_COMPONENTS = 2\n",
    "DISSIMILARITY = 'precomputed'\n",
    "FUZZINESS = 2\n",
    "ERROR_THRESHOLD = 0.005\n",
    "MAX_ITERATIONS = 1000\n",
    "CLUSTER_RANGE = range(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_2D_embedding(distances):\n",
    "    \"\"\"Computes a 2D embedding from the given distance matrix.\"\"\"\n",
    "    embedding = MDS(n_components=N_COMPONENTS, dissimilarity=DISSIMILARITY)\n",
    "    return embedding.fit_transform(distances)\n",
    "\n",
    "def determine_optimal_clusters(data):\n",
    "    \"\"\"Determines the optimal number of clusters using the Elbow method.\"\"\"\n",
    "    fpc_values = []\n",
    "    for c_value in CLUSTER_RANGE:\n",
    "        _, _, _, _, _, _, fpc = fuzz.cmeans(\n",
    "            data.T, \n",
    "            c=c_value, \n",
    "            m=FUZZINESS, \n",
    "            error=ERROR_THRESHOLD, \n",
    "            maxiter=MAX_ITERATIONS\n",
    "        )\n",
    "        fpc_values.append(fpc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(CLUSTER_RANGE, fpc_values)\n",
    "    plt.title('Fuzzy Partition Coefficient (FPC) for different cluster numbers')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('FPC')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return detect_elbow(fpc_values)\n",
    "\n",
    "def detect_elbow(y_values):\n",
    "    \"\"\"Detects the 'elbow' in a list of y-values.\"\"\"\n",
    "    # Get coordinates of all the points\n",
    "    n_points = len(y_values)\n",
    "    all_coords = np.vstack((range(n_points), y_values)).T\n",
    "    # Get vectors between all points from the first point to the last point\n",
    "    first_point = all_coords[0]\n",
    "    line_vector = all_coords[-1] - all_coords[0]\n",
    "    line_vector_norm = line_vector / np.sqrt(np.sum(line_vector**2))\n",
    "    \n",
    "    # Get orthogonal vectors from the first point to all points\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_prod = np.sum(vec_from_first * line_vector_norm, axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_prod, line_vector_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    \n",
    "    # Compute the distance to the line\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    \n",
    "    # Return the index of the point with max distance to the line\n",
    "    idx_elbow = np.argmax(dist_to_line)\n",
    "    return idx_elbow\n",
    "\n",
    "def fuzzy_cmeans_clustering(data, optimal_clusters):\n",
    "    \"\"\"Clusters the data using Fuzzy C-Means.\"\"\"\n",
    "    cntr, u, _, _, _, _, _ = fuzz.cmeans(\n",
    "        data.T, \n",
    "        c=optimal_clusters, \n",
    "        m=FUZZINESS, \n",
    "        error=ERROR_THRESHOLD, \n",
    "        maxiter=MAX_ITERATIONS\n",
    "    )\n",
    "    return np.argmax(u, axis=0)\n",
    "\n",
    "def visualize_clusters(data, labels):\n",
    "    \"\"\"Visualizes the clustered data.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster_num]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Microservice {cluster_num + 1}')\n",
    "\n",
    "    plt.title('Fuzzy C-Means Clustering')\n",
    "    plt.xlabel('MDS Dimension 1')\n",
    "    plt.ylabel('MDS Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_data = compute_2D_embedding(shortest_distances)\n",
    "optimal_clusters = determine_optimal_clusters(shortest_distances)\n",
    "print('Optimal number of clusters: ', optimal_clusters)\n",
    "labels = fuzzy_cmeans_clustering(shortest_distances, optimal_clusters)\n",
    "\n",
    "embedding_2d = MDS(n_components=N_COMPONENTS, dissimilarity=DISSIMILARITY).fit_transform(shortest_distances)\n",
    "visualize_clusters(embedding_2d, labels)\n",
    "\n",
    "# Save results\n",
    "save_microservices_to_file(labels, services_graph, communities_df, f\"results/{version}_{system}_microservices.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Custom fuzzy C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_to_file(clusters, communities_df, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for cluster_num, (center, members) in enumerate(clusters.items(), 1):\n",
    "            file.write(f\"Microservice {cluster_num} centered at {center}:\\n\")\n",
    "            for service in members:\n",
    "                file.write(f\"  - Service: {service}\\n\")\n",
    "                related_classes = communities_df[communities_df['service'] == service]['class_name'].tolist()\n",
    "                for related_class in related_classes:\n",
    "                    file.write(f\"    - Class: {related_class}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyCMeans:\n",
    "    def __init__(self, m, membership_threshold, merge_threshold):\n",
    "        self.m = m  # Fuzziness coefficient\n",
    "        self.threshold = membership_threshold  # Membership threshold\n",
    "        self.merge_threshold = merge_threshold  # Threshold for merging clusters based on overlap\n",
    "        self.coef = 2 / (m - 1)  # Coefficient used in membership calculation\n",
    "\n",
    "    def calculate_membership(self, service_idx, center_idx, distances):\n",
    "        \"\"\"Calculate fuzzy membership of a service to a center.\"\"\"\n",
    "        distance_to_center = distances[service_idx][center_idx]\n",
    "        if distance_to_center == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        membership_sum = 0\n",
    "        for type_center_idx in self.center_type_indices.values():\n",
    "            distance_to_type_center = distances[service_idx][type_center_idx] or 1e9\n",
    "            membership_sum += math.pow(distance_to_center / distance_to_type_center, self.coef)\n",
    "        \n",
    "        return 1 / membership_sum if membership_sum else float('inf')\n",
    "\n",
    "    def cluster_services(self, services_list, distances, service_type):\n",
    "        # Identify service indices of the given type\n",
    "        self.center_type_indices = {\n",
    "            service: idx for idx, service in enumerate(services_list) if service.startswith(service_type)\n",
    "        }\n",
    "\n",
    "        # Initial clustering\n",
    "        clusters = {}\n",
    "        for type_service, type_index in self.center_type_indices.items():\n",
    "            cluster_members = [type_service]  # Initialize cluster with the center itself\n",
    "            for idx, service in enumerate(services_list):\n",
    "                if service != type_service:\n",
    "                    membership_value = self.calculate_membership(idx, type_index, distances)\n",
    "                    if membership_value > self.threshold:\n",
    "                        cluster_members.append(service)\n",
    "            clusters[type_service] = cluster_members\n",
    "\n",
    "        # Remove clusters containing only the center service if that service belongs to another cluster\n",
    "        clusters = self.remove_single_service_clusters(clusters)\n",
    "\n",
    "        # Merge clusters with overlapping services\n",
    "        clusters = self.merge_clusters_based_on_overlap(clusters)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "    def remove_single_service_clusters(self, clusters):\n",
    "        # Find centers that are members of other clusters\n",
    "        centers_in_other_clusters = set()\n",
    "        for center, members in clusters.items():\n",
    "            for member in members:\n",
    "                if member != center and member in self.center_type_indices:\n",
    "                    centers_in_other_clusters.add(member)\n",
    "\n",
    "        # Remove clusters with only the center service if the center belongs to another cluster\n",
    "        clusters_to_remove = [center for center, members in clusters.items() \n",
    "                              if len(members) == 1 and center in centers_in_other_clusters]\n",
    "        for center in clusters_to_remove:\n",
    "            del clusters[center]\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def calculate_overlap(self, cluster_a, cluster_b):\n",
    "        \"\"\"Calculate the overlap between two clusters.\"\"\"\n",
    "        intersection = set(cluster_a).intersection(cluster_b)\n",
    "        union = set(cluster_a).union(cluster_b)\n",
    "        overlap_ratio = len(intersection) / len(union) if union else 0\n",
    "        return overlap_ratio\n",
    "\n",
    "    def merge_clusters_based_on_overlap(self, clusters):\n",
    "        \"\"\"Merge clusters based on the overlap between their services.\"\"\"\n",
    "        clusters_to_merge = []\n",
    "        keys = list(clusters.keys())\n",
    "\n",
    "        # Find pairs of clusters to merge\n",
    "        for i, key_i in enumerate(keys):\n",
    "            for j in range(i + 1, len(keys)):\n",
    "                key_j = keys[j]\n",
    "                if self.calculate_overlap(clusters[key_i], clusters[key_j]) > self.merge_threshold:\n",
    "                    clusters_to_merge.append((key_i, key_j))\n",
    "\n",
    "        # Merge the clusters\n",
    "        for key_i, key_j in clusters_to_merge:\n",
    "            if key_i in clusters and key_j in clusters:\n",
    "                # Extend the services of cluster i with those of cluster j and remove cluster j\n",
    "                clusters[key_i].extend(clusters[key_j])\n",
    "                clusters[key_i] = list(set(clusters[key_i]))  # Remove duplicates\n",
    "                del clusters[key_j]\n",
    "\n",
    "        return clusters\n",
    "\n",
    "# Usage\n",
    "fuzzy_c_means = FuzzyCMeans(m=2, membership_threshold=0.2, merge_threshold=0.15)\n",
    "\n",
    "# Get the list of service nodes from the graph\n",
    "service_nodes = list(services_graph.nodes)\n",
    "\n",
    "# Perform clustering with the best thresholds\n",
    "clusters = fuzzy_c_means.cluster_services(service_nodes, shortest_distances, service_type=\"Application\") # change service type\n",
    "\n",
    "# Output clusters\n",
    "for center, members in clusters.items():\n",
    "    print(f\"Cluster centered at {center}: {members}\")\n",
    "\n",
    "# Save results\n",
    "save_clusters_to_file(clusters, communities_df, f\"results/{version}_{system}_microservices.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

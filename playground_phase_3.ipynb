{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"codebert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert\n",
    "best_community_detection_algorithm = 'Louvain' # All options: Louvain, EdMot, Infomap, LabelPropagation, FastGreedy, GirvanNewman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "embeddings_df = pd.read_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Extract class names and their embeddings\n",
    "class_names = embeddings_df.iloc[:, 0].str.split(';', expand=True)[0]\n",
    "embeddings = embeddings_df.iloc[:, 1:].values\n",
    "class_embeddings_dict = dict(zip(class_names, embeddings))\n",
    "\n",
    "def compute_static_distances_for_service_pairs(class_graph, communities):\n",
    "    merged_df = class_graph.merge(communities, left_on='class1', right_on='class_name').merge(\n",
    "        communities, left_on='class2', right_on='class_name', suffixes=('_1', '_2')\n",
    "    )\n",
    "    inter_service_df = merged_df[merged_df['service_1'] != merged_df['service_2']]\n",
    "    return inter_service_df.groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "\n",
    "def compute_service_embeddings(embeddings_dict, communities):\n",
    "    \"\"\"Compute service embeddings by averaging class embeddings for each service, skipping missing embeddings.\"\"\"\n",
    "    service_embeddings = {}\n",
    "    for service, class_group in communities.groupby('service')['class_name']:\n",
    "        class_embeddings = [embeddings_dict[class_name] for class_name in class_group if class_name in embeddings_dict]\n",
    "        if class_embeddings:\n",
    "            service_embeddings[service] = np.mean(class_embeddings, axis=0)\n",
    "    return service_embeddings\n",
    "\n",
    "def compute_semantic_distances_for_service_pairs(embeddings_dict, communities):\n",
    "    service_embeddings = compute_service_embeddings(embeddings_dict, communities)\n",
    "    \n",
    "    services = list(service_embeddings.keys())\n",
    "    semantic_distances = {}\n",
    "    for i, s1 in enumerate(services):\n",
    "        for j, s2 in enumerate(services):\n",
    "            if i != j:\n",
    "                distance = 1 - spatial.distance.cosine(service_embeddings[s1], service_embeddings[s2])\n",
    "                semantic_distances[(s1, s2)] = distance\n",
    "    \n",
    "    return semantic_distances\n",
    "\n",
    "def normalize_data(data):\n",
    "    min_val, max_val = min(data.values()), max(data.values())\n",
    "    range_val = max_val - min_val\n",
    "    return {k: (v - min_val) / range_val for k, v in data.items()} if range_val else {k: 0 for k, v in data.items()}\n",
    "\n",
    "# Compute static and semantic distances\n",
    "static_distances = compute_static_distances_for_service_pairs(class_graph_df, communities_df)\n",
    "semantic_distances = compute_semantic_distances_for_service_pairs(class_embeddings_dict, communities_df)\n",
    "\n",
    "# Normalize the distances\n",
    "normalized_static_distances = normalize_data(static_distances)\n",
    "normalized_semantic_distances = normalize_data(semantic_distances)\n",
    "\n",
    "# Create the service graph DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_distances.get((s1, s2), 0), normalized_semantic_distances.get((s1, s2), 0)]\n",
    "    for s1, s2 in static_distances.keys()\n",
    "]\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# Save the DataFrame\n",
    "filename = f\"generated_data/graph/service/{version}_{system}_service_graph.csv\"\n",
    "service_graph_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import skfuzzy as fuzz\n",
    "from utils import save_microservices_to_file, save_clusters_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_weight(semantic, static, alpha=0.5):\n",
    "    if not (0 <= semantic <= 1) or not (0 <= static <= 1):\n",
    "        raise ValueError(\"Both 'semantic' and 'static' values should be between 0 and 1.\")\n",
    "    \n",
    "    beta = 1 - alpha\n",
    "    return alpha * static + beta * semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_graph = nx.DiGraph([\n",
    "    (row['service1'], row['service2'], {\"weight\": compute_edge_weight(row['semantic_distance'], row['static_distance'])})\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])\n",
    "\n",
    "# Remove all edges with a weight of 0 to simplify the graph\n",
    "services_graph.remove_edges_from([\n",
    "    (u, v) for u, v, weight in services_graph.edges(data='weight') if weight == 0\n",
    "])\n",
    "\n",
    "# Print nodes and edge weights\n",
    "for u, v, weight in services_graph.edges(data='weight'):\n",
    "    print(f\"{u} -> {v}: {weight}\")\n",
    "\n",
    "# Generate a list of nodes in the graph\n",
    "nodes_list = list(services_graph.nodes)\n",
    "\n",
    "# Initialize a matrix to hold pairwise distances with a default value\n",
    "# indicating maximum dissimilarity\n",
    "default_distance = 1\n",
    "distances = {\n",
    "    node: {node2: default_distance for node2 in nodes_list}\n",
    "    for node in nodes_list\n",
    "}\n",
    "\n",
    "# Update the distance matrix with the inverse of the edge weights\n",
    "for u, v, data in services_graph.edges(data=True):\n",
    "    distances[u][v] = distances[v][u] = 1 - data['weight']\n",
    "\n",
    "# Convert the distances dictionary to a DataFrame and then to a matrix\n",
    "dissimilarity_matrix = pd.DataFrame(distances).values\n",
    "\n",
    "# Set the diagonal of the distance matrix to 0 (distance to self is 0)\n",
    "np.fill_diagonal(dissimilarity_matrix, 0)\n",
    "\n",
    "# Output the dissimilarity matrix\n",
    "print(\"Dissimilarity matrix:\")\n",
    "print(dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Fuzzy C-means from Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FUZZINESS = 2\n",
    "ERROR_THRESHOLD = 0.005\n",
    "MAX_ITERATIONS = 1000\n",
    "CLUSTER_RANGE = range(1, 100)\n",
    "RELATIVE_THRESHOLD = 1  # Membership threshold: adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_clusters(data):\n",
    "    \"\"\"Determines the optimal number of clusters using the Elbow method.\"\"\"\n",
    "    fpc_values = []\n",
    "    for c_value in CLUSTER_RANGE:\n",
    "        _, _, _, _, _, _, fpc = fuzz.cmeans(\n",
    "            data.T, \n",
    "            c=c_value, \n",
    "            m=FUZZINESS, \n",
    "            error=ERROR_THRESHOLD, \n",
    "            maxiter=MAX_ITERATIONS\n",
    "        )\n",
    "        fpc_values.append(fpc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(CLUSTER_RANGE, fpc_values)\n",
    "    plt.title('Fuzzy Partition Coefficient (FPC) for different cluster numbers')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('FPC')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return detect_elbow(fpc_values)\n",
    "\n",
    "def detect_elbow(y_values):\n",
    "    \"\"\"Detects the 'elbow' in a list of y-values.\"\"\"\n",
    "    # Get coordinates of all the points\n",
    "    n_points = len(y_values)\n",
    "    all_coords = np.vstack((range(n_points), y_values)).T\n",
    "    # Get vectors between all points from the first point to the last point\n",
    "    first_point = all_coords[0]\n",
    "    line_vector = all_coords[-1] - all_coords[0]\n",
    "    line_vector_norm = line_vector / np.sqrt(np.sum(line_vector**2))\n",
    "    \n",
    "    # Get orthogonal vectors from the first point to all points\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_prod = np.sum(vec_from_first * line_vector_norm, axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_prod, line_vector_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    \n",
    "    # Compute the distance to the line\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    \n",
    "    # Return the index of the point with max distance to the line\n",
    "    idx_elbow = np.argmax(dist_to_line)\n",
    "    return idx_elbow\n",
    "\n",
    "def fuzzy_cmeans_clustering(data, node_labels, optimal_clusters, relative_threshold=RELATIVE_THRESHOLD):\n",
    "    \"\"\"Clusters the data using Fuzzy C-Means, returning only significant memberships.\"\"\"\n",
    "    cntr, u, _, _, _, _, _ = fuzz.cmeans(\n",
    "        data.T,\n",
    "        c=optimal_clusters,\n",
    "        m=FUZZINESS,\n",
    "        error=ERROR_THRESHOLD,\n",
    "        maxiter=MAX_ITERATIONS\n",
    "    )\n",
    "    # Create a dictionary to store memberships for each node\n",
    "    fuzzy_memberships = {label: [] for label in node_labels}\n",
    "    \n",
    "    # Normalize memberships for each node so that they sum to 1\n",
    "    for i, label in enumerate(node_labels):\n",
    "        membership_values = u[:, i]\n",
    "        total_membership = np.sum(membership_values)\n",
    "        normalized_memberships = membership_values / total_membership if total_membership else membership_values\n",
    "        \n",
    "        # Find the maximum normalized membership value for the node\n",
    "        highest_membership = np.max(normalized_memberships)\n",
    "        \n",
    "        # Include memberships that are above the threshold relative to the highest membership\n",
    "        for j in range(optimal_clusters):\n",
    "            if normalized_memberships[j] / highest_membership >= relative_threshold:\n",
    "                cluster_name = f'cluster{j+1}'\n",
    "                fuzzy_memberships[label].append((cluster_name, normalized_memberships[j]))\n",
    "\n",
    "    return fuzzy_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "optimal_clusters = determine_optimal_clusters(dissimilarity_matrix)\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "fuzzy_clusters = fuzzy_cmeans_clustering(dissimilarity_matrix, nodes_list, optimal_clusters)\n",
    "\n",
    "# Print the fuzzy_clusters in a readable format\n",
    "for node, memberships in fuzzy_clusters.items():\n",
    "    membership_str = ', '.join(f\"{cluster}: {membership:.2f}\" for cluster, membership in memberships)\n",
    "    print(f\"{node}: [{membership_str}]\")\n",
    "\n",
    "# Save results\n",
    "# You will need to modify the save_microservices_to_file function to handle the new format\n",
    "# save_microservices_to_file(fuzzy_clusters, services_graph, communities_df, f\"results/{version}_{system}_microservices.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Custom fuzzy C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyCMeans:\n",
    "    def __init__(self, m, membership_threshold, merge_threshold):\n",
    "        \"\"\"\n",
    "        Initialize the FuzzyCMeans object with the specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        m (float): The fuzziness coefficient that determines the level of cluster fuzziness.\n",
    "        membership_threshold (float): The proportion of the highest membership value \n",
    "                                      used as a cutoff for assigning services to additional clusters.\n",
    "        merge_threshold (float): The threshold for determining when to merge two clusters\n",
    "                                 based on the overlap of their members.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.membership_threshold = membership_threshold\n",
    "        self.merge_threshold = merge_threshold\n",
    "        self.coef = 2 / (m - 1)  # Precomputed coefficient for membership calculation.\n",
    "\n",
    "    def calculate_membership(self, service_idx, center_idx, distances):\n",
    "        \"\"\"\n",
    "        Calculate the fuzzy membership of a service to a cluster center.\n",
    "        \n",
    "        Parameters:\n",
    "        service_idx (int): Index of the service for which membership is being calculated.\n",
    "        center_idx (int): Index of the cluster center to which membership is being calculated.\n",
    "        distances (list): Matrix of distances between services and cluster centers.\n",
    "        \n",
    "        Returns:\n",
    "        float: The membership value of the service to the cluster center.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10  # Small value to avoid division by zero.\n",
    "        distance_to_center = distances[service_idx][center_idx]\n",
    "\n",
    "        membership_sum = 0.0\n",
    "\n",
    "        if distance_to_center == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        for other_center_idx in self.center_type_indices.values():\n",
    "            if other_center_idx != service_idx:\n",
    "                # Direct distances to other centers\n",
    "                other_distance_to_center = distances[service_idx][other_center_idx]\n",
    "\n",
    "                membership_ratio = distance_to_center / (other_distance_to_center + epsilon)\n",
    "                membership_sum += membership_ratio ** self.coef\n",
    "\n",
    "        # The membership value is the inverse of the sum, normalized by the number of centers\n",
    "        return 1 / membership_sum if membership_sum > 0 else 0\n",
    "\n",
    "        \n",
    "    def cluster_services(self, services_list, distances, service_type):\n",
    "        \"\"\"\n",
    "        Cluster services based on their membership values to each cluster center.\n",
    "        \n",
    "        Parameters:\n",
    "        services_list (list): List of all services to be clustered.\n",
    "        distances (list): Matrix of distances between services and cluster centers.\n",
    "        service_type (str): The type of service that defines the cluster centers.\n",
    "        \n",
    "        Returns:\n",
    "        dict: A dictionary with cluster centers as keys and lists of services as values.\n",
    "        \"\"\"\n",
    "        # Step 1: Initialize clusters and identify services that are centers based on the service type.\n",
    "        self.center_type_indices = {\n",
    "            service: idx for idx, service in enumerate(services_list) if service.startswith(service_type)\n",
    "        }\n",
    "\n",
    "        # Step 2: Calculate raw membership values for each service to each cluster center.\n",
    "        raw_memberships = {\n",
    "            service: [] for service in services_list\n",
    "        }\n",
    "        for idx, service in enumerate(services_list):\n",
    "            for center_service, center_idx in self.center_type_indices.items():\n",
    "                membership_value = self.calculate_membership(idx, center_idx, distances)\n",
    "                raw_memberships[service].append((center_service, membership_value))\n",
    "\n",
    "        # Step 3: Normalize membership values so that they sum to 1 for each service.\n",
    "        normalized_memberships = {}\n",
    "        for service, memberships in raw_memberships.items():\n",
    "            total_membership = sum(membership for _, membership in memberships)\n",
    "            normalized_memberships[service] = [\n",
    "                (center_service, membership / total_membership if total_membership else 0)\n",
    "                for center_service, membership in memberships\n",
    "            ]\n",
    "\n",
    "        # Print the normalized memberships for each service.\n",
    "        # for service, memberships in normalized_memberships.items():\n",
    "        #     print(f\"{service}: {memberships}\")\n",
    "\n",
    "        # Step 4: Assign services to clusters based on normalized membership values.\n",
    "        clusters = {center_service: [] for center_service in self.center_type_indices}\n",
    "        for service, memberships in normalized_memberships.items():\n",
    "            # Determine the highest normalized membership value.\n",
    "            highest_membership_value = max(memberships, key=lambda x: x[1])[1]\n",
    "            # Determine the primary cluster for this service.\n",
    "            primary_cluster = max(memberships, key=lambda x: x[1])[0]\n",
    "            clusters[primary_cluster].append(service)\n",
    "            \n",
    "            # Assign service to other clusters where the membership value is above the relative threshold.\n",
    "            relative_threshold = highest_membership_value * self.membership_threshold\n",
    "            for center_service, membership_value in memberships:\n",
    "                if membership_value >= relative_threshold and center_service != primary_cluster:\n",
    "                    clusters[center_service].append(service)\n",
    "\n",
    "        # Step 5: Merge clusters if they have a significant overlap as defined by merge_threshold.\n",
    "        clusters = self.merge_clusters_based_on_overlap(clusters)\n",
    "\n",
    "        # Step 6: Remove duplicate services from each cluster to maintain unique membership.\n",
    "        for center_service, members in clusters.items():\n",
    "            clusters[center_service] = list(set(members))\n",
    "\n",
    "        # Step 7: Output the clusters in correct format.\n",
    "        # First, create a dictionary to map cluster labels to indices.\n",
    "        cluster_indices = {cluster: idx for idx, cluster in enumerate(clusters.keys())}\n",
    "\n",
    "        # Then, create a dictionary to store the cluster memberships for each node.\n",
    "        fuzzy_memberships = {node: [] for node in services_list}\n",
    "\n",
    "        # Finally, populate the node_memberships dictionary with normalized membership values.\n",
    "        for node, memberships in normalized_memberships.items():\n",
    "            for cluster, membership in memberships:\n",
    "                    if cluster in clusters and node in clusters[cluster]:\n",
    "                        fuzzy_memberships[node].append(('cluster' + str(cluster_indices[cluster]), membership))\n",
    "\n",
    "        return fuzzy_memberships\n",
    "\n",
    "\n",
    "    def calculate_overlap(self, cluster_a, cluster_b):\n",
    "        \"\"\"\n",
    "        Calculate the overlap between two clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        cluster_a (list): The first cluster to compare.\n",
    "        cluster_b (list): The second cluster to compare.\n",
    "        \n",
    "        Returns:\n",
    "        float: The ratio of the intersection to the union of the two clusters.\n",
    "        \"\"\"\n",
    "        intersection = set(cluster_a).intersection(cluster_b)\n",
    "        union = set(cluster_a).union(cluster_b)\n",
    "        return len(intersection) / len(union) if union else 0\n",
    "\n",
    "    def merge_clusters_based_on_overlap(self, clusters):\n",
    "        \"\"\"\n",
    "        Merge clusters based on the overlap between their services.\n",
    "        \n",
    "        Parameters:\n",
    "        clusters (dict): Dictionary with cluster centers as keys and lists of services as values.\n",
    "        \n",
    "        Returns:\n",
    "        dict: The updated dictionary of clusters after merging.\n",
    "        \"\"\"\n",
    "        # Find all pairs of clusters that have an overlap above the merge threshold.\n",
    "        clusters_to_merge = []\n",
    "        keys = list(clusters.keys())\n",
    "        for i, key_i in enumerate(keys):\n",
    "            for j in range(i + 1, len(keys)):\n",
    "                key_j = keys[j]\n",
    "                if self.calculate_overlap(clusters[key_i], clusters[key_j]) > self.merge_threshold:\n",
    "                    clusters_to_merge.append((key_i, key_j))\n",
    "\n",
    "        # Merge the identified clusters.\n",
    "        for key_i, key_j in clusters_to_merge:\n",
    "            if key_i in clusters and key_j in clusters:\n",
    "                clusters[key_i].extend(clusters[key_j])\n",
    "                clusters[key_i] = list(set(clusters[key_i]))\n",
    "                del clusters[key_j]\n",
    "\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (change if needed)\n",
    "FUZZINESS = 2\n",
    "RELATIVE_MEMBERSHIP_THRESHOLD = 0.7 # lower means clustering more loosely (services can be in multiple clusters)\n",
    "MERGE_THRESHOLD = 0.5 # higher means less clusters will be merged\n",
    "CENTER_TYPE = 'Entity Service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "fuzzy_c_means = FuzzyCMeans(m=FUZZINESS, membership_threshold=RELATIVE_MEMBERSHIP_THRESHOLD, merge_threshold=MERGE_THRESHOLD) # change thresholds\n",
    "\n",
    "# Perform clustering with the best thresholds\n",
    "fuzzy_clusters = fuzzy_c_means.cluster_services(nodes_list, dissimilarity_matrix, service_type=CENTER_TYPE) # change service type\n",
    "\n",
    "# Output clusters\n",
    "print(fuzzy_clusters)\n",
    "\n",
    "# Save results\n",
    "# save_clusters_to_file(clusters, communities_df, f\"results/{version}_{system}_microservices.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def hierarchical_fuzzy_membership(dissimilarity_matrix, nodes, max_d, relative_threshold):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering and return fuzzy memberships.\n",
    "\n",
    "    Parameters:\n",
    "    - dissimilarity_matrix: The matrix of dissimilarity between nodes.\n",
    "    - nodes: The list of node labels.\n",
    "    - max_d: The threshold to cut the dendrogram to form clusters.\n",
    "    - relative_threshold: The relative threshold for determining significant membership.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with node labels as keys and a list of (cluster, membership) tuples as values.\n",
    "    \"\"\"\n",
    "    # Perform the hierarchical clustering\n",
    "    Z = linkage(squareform(dissimilarity_matrix), method='average')\n",
    "    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "\n",
    "    # Calculate the centroids of each cluster\n",
    "    centroids = {i: np.mean(dissimilarity_matrix[clusters == i], axis=0) for i in np.unique(clusters)}\n",
    "\n",
    "    # Initialize the fuzzy memberships dictionary\n",
    "    fuzzy_memberships = {}\n",
    "\n",
    "    for node_idx, node in enumerate(nodes):\n",
    "        # Calculate distances to centroids\n",
    "        distances = {f'cluster{i}': np.linalg.norm(centroids[i] - dissimilarity_matrix[node_idx]) for i in centroids}\n",
    "        \n",
    "        # Invert the distances to get membership strengths (higher distance -> lower membership)\n",
    "        inverted_memberships = {cluster_label: 1 / (distance + 1e-5) for cluster_label, distance in distances.items()}\n",
    "        \n",
    "        # Normalize the inverted memberships so they sum to 1 for each node\n",
    "        total_inverted = sum(inverted_memberships.values())\n",
    "        normalized_memberships = {cluster_label: membership / total_inverted for cluster_label, membership in inverted_memberships.items()}\n",
    "        \n",
    "        # Determine the maximum membership value for the node\n",
    "        max_membership = max(normalized_memberships.values())\n",
    "        \n",
    "        # Include memberships that are above the relative threshold\n",
    "        significant_memberships = {\n",
    "            cluster_label: membership\n",
    "            for cluster_label, membership in normalized_memberships.items()\n",
    "            if membership / max_membership >= relative_threshold\n",
    "        }\n",
    "        \n",
    "        # Add the significant memberships to the fuzzy_memberships dictionary for the node\n",
    "        fuzzy_memberships[node] = list(significant_memberships.items())\n",
    "\n",
    "    return fuzzy_memberships\n",
    "\n",
    "fuzzy_memberships = hierarchical_fuzzy_membership(dissimilarity_matrix, nodes_list, max_d=0.55, relative_threshold=0.9)\n",
    "\n",
    "# Print the fuzzy_memberships in a readable format\n",
    "for node, memberships in fuzzy_memberships.items():\n",
    "    membership_str = ', '.join(f\"{cluster}: {membership:.2f}\" for cluster, membership in memberships)\n",
    "    print(f\"{node}: [{membership_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modularity-based Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "def convert_fuzzy_to_hard_partition(fuzzy_clusters):\n",
    "    hard_partition = {}\n",
    "    for node, memberships in fuzzy_clusters.items():\n",
    "        # Assign the node to the cluster with the highest membership degree\n",
    "        highest_membership_cluster = max(memberships, key=lambda x: x[1])[0]\n",
    "        hard_partition.setdefault(highest_membership_cluster, set()).add(node)\n",
    "    return list(hard_partition.values())\n",
    "\n",
    "def optimize_clustering_parameters(graph, dissimilarity_matrix, clustering_function, parameter_ranges):\n",
    "    \"\"\"\n",
    "    Optimize clustering parameters based on modularity.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: The graph on which the clustering is to be performed.\n",
    "    - distance_matrix: The precomputed dissimilarity matrix for the graph.\n",
    "    - clustering_function: The clustering function to be optimized.\n",
    "    - parameter_ranges: A dictionary where keys are parameter names and values are lists of parameter values to try.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the optimal parameters and the corresponding modularity score.\n",
    "    \"\"\"\n",
    "    best_modularity = -1\n",
    "    best_parameters = None\n",
    "\n",
    "    # Convert directed graph to undirected for modularity calculation\n",
    "    if graph.is_directed():\n",
    "        graph = graph.to_undirected()\n",
    "    \n",
    "    # Generate all combinations of parameters\n",
    "    keys, values = zip(*parameter_ranges.items())\n",
    "    for parameter_combination in itertools.product(*values):\n",
    "        params = dict(zip(keys, parameter_combination))\n",
    "\n",
    "        # Perform clustering with the given parameters\n",
    "        fuzzy_clusters = clustering_function(dissimilarity_matrix, **params)\n",
    "\n",
    "        # Convert fuzzy clusters to a hard partition\n",
    "        communities = convert_fuzzy_to_hard_partition(fuzzy_clusters)\n",
    "\n",
    "        # Calculate modularity\n",
    "        current_modularity = modularity(graph, communities)\n",
    "\n",
    "        # Update best parameters if current modularity is greater than the best found so far\n",
    "        if current_modularity > best_modularity:\n",
    "            best_modularity = current_modularity\n",
    "            best_parameters = params\n",
    "\n",
    "    return best_parameters, best_modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_cmeans_params = {\n",
    "    'optimal_clusters': range(2, 100),\n",
    "}\n",
    "best_fuzzy_cmeans_params, best_fuzzy_cmeans_modularity = optimize_clustering_parameters(\n",
    "    services_graph,\n",
    "    dissimilarity_matrix,\n",
    "    fuzzy_cmeans_clustering,\n",
    "    fuzzy_cmeans_params\n",
    ")\n",
    "\n",
    "# Example usage for Custom fuzzy C-means\n",
    "custom_fuzzy_cmeans_params = {\n",
    "    'm': range(2, 5),\n",
    "    'membership_threshold': np.linspace(0.1, 1.0, 20),\n",
    "    'merge_threshold': np.linspace(0.1, 1.0, 20),\n",
    "}\n",
    "best_custom_fuzzy_cmeans_params, best_custom_fuzzy_cmeans_modularity = optimize_clustering_parameters(\n",
    "    services_graph,\n",
    "    dissimilarity_matrix,\n",
    "    lambda d, **params: FuzzyCMeans(**params).cluster_services(nodes_list, d, CENTER_TYPE),\n",
    "    custom_fuzzy_cmeans_params\n",
    ")\n",
    "\n",
    "# Example usage for Hierarchical clustering\n",
    "hierarchical_params = {\n",
    "    'max_d': np.linspace(0.0, 1.0, 20)\n",
    "}\n",
    "best_hierarchical_params, best_hierarchical_modularity = optimize_clustering_parameters(\n",
    "    services_graph,\n",
    "    dissimilarity_matrix,\n",
    "    lambda d, **params: fcluster(linkage(squareform(d), method='average'), **params),\n",
    "    hierarchical_params\n",
    ")\n",
    "\n",
    "# Print the best parameters and their corresponding modularity scores\n",
    "print(\"Fuzzy C-means:\")\n",
    "print(f\"Best parameters: {best_fuzzy_cmeans_params}\")\n",
    "print(f\"Best modularity: {best_fuzzy_cmeans_modularity}\")\n",
    "print()\n",
    "print(\"Custom fuzzy C-means:\")\n",
    "print(f\"Best parameters: {best_custom_fuzzy_cmeans_params}\")\n",
    "print(f\"Best modularity: {best_custom_fuzzy_cmeans_modularity}\")\n",
    "print()\n",
    "print(\"Hierarchical clustering:\")\n",
    "print(f\"Best parameters: {best_hierarchical_params}\")\n",
    "print(f\"Best modularity: {best_hierarchical_modularity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert\n",
    "\n",
    "\n",
    "\n",
    "best_community_detection_algorithm = 'Louvain' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data saved to generated_data/graph/service/v_imen_pos_service_graph.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_balanced_weight(static_weight,semantic_weight):\n",
    "    weight_relation=0.5\n",
    "    weight_sementic=0.5\n",
    "    return weight_relation* static_weight + weight_sementic*semantic_weight\n",
    "\n",
    "# Function to calculate service embedding\n",
    "def calculate_service_embedding(service_classes):\n",
    "    return sum(class_embeddings_dict[class_name] for class_name in service_classes) / len(service_classes)\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = communities_df.groupby('service')['class_name'].apply(lambda x: calculate_service_embedding(x)).to_dict()\n",
    "\n",
    "# Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(service_to_embedding[s1], service_to_embedding[s2])\n",
    "        for s2 in service_to_embedding if s1 != s2\n",
    "    }\n",
    "    for s1 in service_to_embedding\n",
    "}\n",
    "\n",
    "service_graph_data = []\n",
    "for row in class_graph_df.itertuples(index=False):\n",
    "    service1 = communities_df.loc[communities_df['class_name'] == row.class1, 'service'].values[0]\n",
    "    service2 = communities_df.loc[communities_df['class_name'] == row.class2, 'service'].values[0]\n",
    "    \n",
    "    if service1 == service2:\n",
    "        continue\n",
    "    \n",
    "    static_distance = row.static_distance\n",
    "    semantic_distance = service_similarities.get(service1, {}).get(service2)\n",
    "\n",
    "\n",
    "    if semantic_distance is not None:\n",
    "        service_graph_data.append([service1, service2, static_distance, semantic_distance])\n",
    "    \n",
    "\n",
    "# Create service_graph_df DataFrame\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# Output service_graph_df\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv_name = f\"generated_data/graph/service/{version}_{system}_service_graph.csv\"\n",
    "# Save service_graph_df to CSV (we may need to add the community detection algo in the filename)\n",
    "service_graph_df.to_csv(csv_name, index=False)\n",
    "\n",
    "print(f\"Graph data saved to {csv_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix saved to generated_data/graph/service/v_imen_pos_adjacency_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a graph and add edges\n",
    "GServices = nx.Graph()\n",
    "for d in service_graph_data:\n",
    "    GServices.add_edge(d[0], d[1], weight=calculate_balanced_weight(d[2], d[3]))\n",
    "\n",
    "node_list = list(GServices.nodes())\n",
    "\n",
    "# Create an empty adjacency matrix filled with zeros\n",
    "num_nodes = len(node_list)\n",
    "adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "# Populate the adjacency matrix with edge weights\n",
    "for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "        if i == j:\n",
    "            # Diagonal elements (self-loops) can be set to zero or any other appropriate value\n",
    "            adjacency_matrix[i][j] = 0\n",
    "        else:\n",
    "            # Check if there is an edge between the nodes\n",
    "            if GServices.has_edge(node_list[i], node_list[j]):\n",
    "                # Get the weight of the edge\n",
    "                edge_data = GServices[node_list[i]][node_list[j]]\n",
    "                edge_weight = edge_data['weight']\n",
    "                adjacency_matrix[i][j] = edge_weight\n",
    "\n",
    "# Create a DataFrame for the adjacency matrix with service names\n",
    "adjacency_df = pd.DataFrame(adjacency_matrix, index=node_list, columns=node_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = f\"generated_data/graph/service/{version}_{system}_adjacency_matrix.csv\"\n",
    "adjacency_df.to_csv(csv_filename)\n",
    "\n",
    "print(f\"Adjacency matrix saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpc_score for each number of MS\n",
      "[119.87955250854776, 131.32147675318163, 141.84340448569552, 151.63698668259616, 160.8353004893823]\n",
      "Optimal number of MS\n",
      "9\n",
      "Microservices assignments saved to generated_data/graph/service/v_imen_pos_microservices.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Define a range of cluster numbers to consider\n",
    "num_clusters_range = range(5, 10)  # Adjust the range as needed\n",
    "\n",
    "# Initialize empty lists to store the results\n",
    "fcm_scores = []\n",
    "fpc_scores = []\n",
    "\n",
    "# Calculate Fuzzy C-Means scores and FPC scores for different cluster numbers\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Specify the fuzziness coefficient (e.g., m=2.0)\n",
    "    m = 3.0\n",
    "    fcm = fuzz.cmeans(adjacency_matrix, num_clusters, m, error=0.005, maxiter=1000)\n",
    "    fcm_scores.append(fcm[3])  # Appending the mean squared error (MSE) to scores\n",
    "\n",
    "    # Calculate FPC score using the formula FPC = (Tr(B) / W) / (1 - Tr(W) / W)\n",
    "    B = np.linalg.norm(fcm[0], axis=0)\n",
    "    W = np.mean(fcm[3])\n",
    "    fpc = (np.sum(B) / W) / (1 - W)\n",
    "    fpc_scores.append(fpc)\n",
    "\n",
    "print(\"fpc_score for each number of MS\")\n",
    "print(fpc_scores)\n",
    "# Find the number of clusters with the highest FPC score\n",
    "\n",
    "\n",
    "optimal_num_clusters = num_clusters_range[np.argmax(fpc_scores)]\n",
    "\n",
    "print(\"Optimal number of MS\")\n",
    "print(optimal_num_clusters)\n",
    "\n",
    "# Apply Fuzzy C-Means clustering with the optimal number of clusters\n",
    "fcm = fuzz.cmeans(adjacency_matrix, optimal_num_clusters, m, error=0.005, maxiter=1000)\n",
    "\n",
    "# Get cluster memberships for each Application Service\n",
    "membership_degrees = fcm[1]\n",
    "\n",
    "# Calculate the threshold dynamically based on the mean membership degree\n",
    "mean_membership_degrees = np.mean(membership_degrees, axis=1)\n",
    "threshold = np.mean(mean_membership_degrees)\n",
    "\n",
    "# Create a dictionary to map Application Services to their clusters\n",
    "service_clusters = {}\n",
    "for i, service in enumerate(GServices.nodes()):\n",
    "    for cluster in range(optimal_num_clusters):\n",
    "        if membership_degrees[cluster][i] > threshold:\n",
    "            if cluster not in service_clusters:\n",
    "                service_clusters[cluster] = []\n",
    "            service_clusters[cluster].append(service)\n",
    "\n",
    "output_file_path = f\"generated_data/graph/service/{version}_{system}_microservices.txt\"\n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for ms, s in service_clusters.items():\n",
    "        file.write(f\"Microservice {ms + 1}:\\n\")\n",
    "        for service in s:\n",
    "            file.write(f\"  - {service}\\n\")\n",
    "\n",
    "print(\"Microservices assignments saved to\", output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

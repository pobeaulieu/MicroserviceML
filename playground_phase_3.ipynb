{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Microservice Identification (Grouping by Similar Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create service graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from utils import load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV to DataFrame\n",
    "best_community_detection_algorithm = 'EdMot' # Change this\n",
    "communities_df = pd.read_csv(f\"generated_data/community/{version}_{system}_{best_community_detection_algorithm}_communities.csv\")\n",
    "class_graph_df = pd.read_csv(f\"generated_data/graph/class/{version}_{system}_class_graph.csv\")\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Data Structuring\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate service embeddings\n",
    "\n",
    "# Filter out class names not present in the embeddings dictionary\n",
    "valid_communities_df = communities_df[communities_df['class_name'].isin(class_embeddings_dict.keys())]\n",
    "\n",
    "# Calculate service embeddings\n",
    "service_to_embedding = valid_communities_df.groupby('service')['class_name'].apply(\n",
    "    lambda x: sum(class_embeddings_dict[class_name] for class_name in x) / len(x)\n",
    ").to_dict()\n",
    "\n",
    "# 2. Calculate service similarities\n",
    "service_similarities = {\n",
    "    s1: {\n",
    "        s2: 1 - spatial.distance.cosine(embedding1, embedding2)\n",
    "        for s2, embedding2 in service_to_embedding.items() if s1 != s2\n",
    "    }\n",
    "    for s1, embedding1 in service_to_embedding.items()\n",
    "}\n",
    "\n",
    "# 3. Create dictionaries to store processed distances using merges\n",
    "merged_df = class_graph_df.merge(\n",
    "    communities_df, left_on='class1', right_on='class_name', how='inner'\n",
    ").merge(\n",
    "    communities_df, left_on='class2', right_on='class_name', how='inner', suffixes=('_1', '_2')\n",
    ")\n",
    "\n",
    "# Filter rows where services are the same and accumulate distances\n",
    "static_dict = merged_df.loc[merged_df['service_1'] != merged_df['service_2']].groupby(['service_1', 'service_2'])['static_distance'].sum().to_dict()\n",
    "semantic_dict = {(s1, s2): service_similarities.get(s1, {}).get(s2) for s1, s2 in static_dict.keys()}\n",
    "\n",
    "# 4. Normalize static distances\n",
    "max_static_distance = max(static_dict.values()) if static_dict else 0\n",
    "normalized_static_dict = {k: v / max_static_distance for k, v in static_dict.items()}\n",
    "\n",
    "# 5. Create the service_graph_df DataFrame\n",
    "service_graph_data = [\n",
    "    [s1, s2, normalized_static_dict[(s1, s2)], semantic_dict.get((s1, s2), 0)]\n",
    "    for s1, s2 in normalized_static_dict.keys()\n",
    "]\n",
    "\n",
    "service_graph_df = pd.DataFrame(service_graph_data, columns=['service1', 'service2', 'static_distance', 'semantic_distance'])\n",
    "\n",
    "# 6. Save service_graph_df to CSV\n",
    "service_graph_df.to_csv(f\"generated_data/graph/service/{version}_{system}_service_graph.csv\", index=False)\n",
    "\n",
    "service_graph_df.head()  # Display the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_weight(semantic, static):\n",
    "    semantic_weight = 1 / semantic if semantic != 0 else 0 # why\n",
    "    return 0.1 * static + 100 * semantic_weight\n",
    "\n",
    "# Compute edges with weights and construct the graph\n",
    "services_graph = nx.Graph([\n",
    "    (row['service1'], row['service2'], {\"weight\": edge_weight(row['semantic_distance'], row['static_distance'])})\n",
    "    for _, row in service_graph_df.iterrows()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "def create_adjacency_matrix(graph, df):\n",
    "    \"\"\"Create a directed adjacency matrix from service graph and dataframe.\"\"\"\n",
    "    nodes = list(graph.nodes)\n",
    "    node_to_index = {node: idx for idx, node in enumerate(nodes)}\n",
    "    \n",
    "    # Initialize the matrix with inf values\n",
    "    matrix = np.full((len(nodes), len(nodes)), np.inf) # why\n",
    "    \n",
    "    # Extract the source, destination and static distance columns\n",
    "    src_indices = df['service1'].map(node_to_index).values\n",
    "    dest_indices = df['service2'].map(node_to_index).values\n",
    "    static_dists = df['static_distance'].replace(0, np.inf).values\n",
    "    \n",
    "    # Use the indices for efficient assignment\n",
    "    matrix[src_indices, dest_indices] = 100 / static_dists # why\n",
    "\n",
    "    return matrix, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floyd Warshall Algorithm\n",
    "def floyd_warshall(adj_matrix):\n",
    "    \"\"\"Compute shortest paths for all pairs of nodes using the Floyd Warshall algorithm (optimized with numpy).\"\"\"\n",
    "    distance = adj_matrix.copy()\n",
    "    num_vertices = distance.shape[0]\n",
    "    \n",
    "    for k in range(num_vertices):\n",
    "        distance = np.minimum(distance, distance[:, k][:, np.newaxis] + distance[np.newaxis, :, k])\n",
    "                \n",
    "    return np.array(distance)\n",
    "\n",
    "# Alternative: Use networkx's floyd_warshall_numpy function for shortest paths computation\n",
    "# shortest_distances = nx.floyd_warshall_numpy(services_graph, weight='weight') # skips the need for adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adjacency matrix\n",
    "adj_matrix, nodes_list = create_adjacency_matrix(services_graph, service_graph_df)\n",
    "print(adj_matrix)\n",
    "\n",
    "# Calculate shortest paths using Floyd Warshall\n",
    "shortest_distances = floyd_warshall(adj_matrix)\n",
    "print(shortest_distances)\n",
    "\n",
    "# Update with semantic component\n",
    "for i, service_i in enumerate(nodes_list):\n",
    "    for j, service_j in enumerate(nodes_list):\n",
    "        distance = service_graph_df[\n",
    "            (service_graph_df['service1'] == service_i) & \n",
    "            (service_graph_df['service2'] == service_j)\n",
    "        ]['semantic_distance'].iloc[0] if not service_graph_df[\n",
    "            (service_graph_df['service1'] == service_i) & \n",
    "            (service_graph_df['service2'] == service_j)\n",
    "        ]['semantic_distance'].empty else 0\n",
    "        \n",
    "        shortest_distances[i][j] += distance * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fuzzy_weight(service_idx, center_idx, distances, centers):\n",
    "    \"\"\"\n",
    "    Calculate the fuzzy weight for a service in relation to a center node.\n",
    "    \"\"\"\n",
    "    m = 3\n",
    "    coef = 2 / (m - 1)\n",
    "    \n",
    "    service_to_center_distance = distances[service_idx][center_idx]\n",
    "\n",
    "    if service_to_center_distance == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    normalized_distance_sum = sum(\n",
    "        service_to_center_distance / (distances[service_idx][center_index] if distances[service_idx][center_index] != 0 else float('inf'))\n",
    "        for center_index in centers.values()\n",
    "    )\n",
    "\n",
    "    return 1 / math.pow(normalized_distance_sum, coef)\n",
    "\n",
    "# Identify center nodes and their indices\n",
    "center_type_prefix = \"Application\"  # Change this prefix as needed to adapt for different types of nodes\n",
    "centers = {node_name: idx for idx, node_name in enumerate(services_graph.nodes) if node_name.startswith(center_type_prefix)}\n",
    "\n",
    "# Map centers to related services based on the fuzzy weights\n",
    "center_to_services = {}\n",
    "\n",
    "for center_name, center_idx in centers.items():\n",
    "    related_services = [\n",
    "        service_name \n",
    "        for service_idx, service_name in enumerate(services_graph.nodes) \n",
    "        if service_name != center_name and calculate_fuzzy_weight(service_idx, center_idx, shortest_distances, centers) * 100 > 9\n",
    "    ]\n",
    "\n",
    "    center_to_services[center_name] = related_services\n",
    "\n",
    "print(center_to_services)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Type-based Service Identification (Grouping by Same Type Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"albert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create class graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from utils import write_call_graph_to_csv, load_class_code_from_directory, load_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_code_dict = load_class_code_from_directory(system)\n",
    "class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "\n",
    "# Create 2 dicts: one for class labels and one for class embeddings using class names as keys\n",
    "class_labels_dict = dict(zip(class_names, class_labels))\n",
    "class_embeddings_dict = dict(zip(class_names, class_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Generate call graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: à remplacer par la méthode d'Imen de génération des dépendances\n",
    "# Counts the number of dependencies between classes in both directions\n",
    "def extract_dependencies_from_class_code(class_code_dict):\n",
    "    all_classes = set(class_code_dict.keys())\n",
    "    dependencies = {}\n",
    "\n",
    "    for class_name, content in class_code_dict.items():\n",
    "        # Count dependencies for the current class code\n",
    "        for target_class in all_classes:\n",
    "            if class_name != target_class:\n",
    "                # The pattern r'{}\\s*[.\\(]' searches for the target class's name followed by a space, period (.), or an opening parenthesis (().\n",
    "                # This is to avoid counting dependencies for classes that have similar names (e.g. \"User\" and \"UserDetails\")\n",
    "                count = len(re.findall(r'{}\\s*[.\\(]'.format(target_class.split('.')[-1]), content))\n",
    "                if count > 0:\n",
    "                    if (class_name, target_class) in dependencies:\n",
    "                        dependencies[(class_name, target_class)] += count\n",
    "                    else:\n",
    "                        dependencies[(class_name, target_class)] = count\n",
    "    \n",
    "    return all_classes, dependencies\n",
    "\n",
    "def compute_distance_matrix(all_classes, dependencies):\n",
    "    matrix = {}\n",
    "    for class1 in all_classes:\n",
    "        matrix[class1] = {}\n",
    "        for class2 in all_classes:\n",
    "            if class1 == class2:\n",
    "                matrix[class1][class2] = 0\n",
    "            else:\n",
    "                matrix[class1][class2] = dependencies.get((class1, class2), 0) + dependencies.get((class2, class1), 0)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes, dependencies = extract_dependencies_from_class_code(class_code_dict)\n",
    "distance_matrix = compute_distance_matrix(all_classes, dependencies)\n",
    "write_call_graph_to_csv(distance_matrix, version, system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Compute static distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"./generated_data/graph/call/{version}_{system}_call_graph.csv\"\n",
    "\n",
    "# Read the dependency graph CSV file into a DataFrame\n",
    "dependency_graph_df = pd.read_csv(\n",
    "    file_path,\n",
    "    delimiter=',',\n",
    "    header=None,\n",
    "    skiprows=1,  # Skip the first row which contains the header\n",
    "    names=['class1', 'class2', 'static_distance']\n",
    ")\n",
    "\n",
    "# Optional: Normalize structural distances\n",
    "static_distance_min = dependency_graph_df['static_distance'].min()\n",
    "static_distance_max = dependency_graph_df['static_distance'].max()\n",
    "\n",
    "dependency_graph_df['static_distance'] = (dependency_graph_df['static_distance'] - static_distance_min) / \\\n",
    "                                  (static_distance_max - static_distance_min)\n",
    "\n",
    "# Create the static_df with the 'static_distance' column\n",
    "static_df = dependency_graph_df[['class1', 'class2', 'static_distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Compute semantic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise semantic distances\n",
    "semantic_distances = []\n",
    "for class_name1 in class_embeddings_dict:\n",
    "    for class_name2 in class_embeddings_dict:\n",
    "        distance = 1 - spatial.distance.cosine(class_embeddings_dict[class_name1], class_embeddings_dict[class_name2])\n",
    "        semantic_distances.append([class_name1, class_name2, distance])\n",
    "\n",
    "semantic_df = pd.DataFrame(semantic_distances, columns=['class1', 'class2', 'semantic_distance'])\n",
    "\n",
    "# Filter rows where one of the class names is not in class_labels keys\n",
    "class_label_keys = set(class_labels_dict.keys())\n",
    "semantic_df = semantic_df[(semantic_df['class1'].isin(class_label_keys)) & (semantic_df['class2'].isin(class_label_keys))]\n",
    "\n",
    "# Normalize semantic distances\n",
    "semantic_df['semantic_distance'] = (semantic_df['semantic_distance'] - semantic_df['semantic_distance'].min()) / \\\n",
    "                                   (semantic_df['semantic_distance'].max() - semantic_df['semantic_distance'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Visualize distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge structural and semantic dataframes based on class1 and class2 columns\n",
    "class_graph = static_df.merge(semantic_df, on=['class1', 'class2'], how='outer')\n",
    "\n",
    "# Fill NA values (if any) for both static_distance and semantic_distance with zeros\n",
    "class_graph.fillna({'static_distance': 0, 'semantic_distance': 0}, inplace=True)\n",
    "\n",
    "# Visualize the static distances\n",
    "static_pivot_table = class_graph.pivot(index='class1', columns='class2', values='static_distance')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(static_pivot_table, cmap='coolwarm', cbar_kws={'label': 'Static Distance'})\n",
    "plt.title(\"Static Distances Between Classes\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the semantic distances\n",
    "semantic_pivot_table = class_graph.pivot(index='class1', columns='class2', values='semantic_distance')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(semantic_pivot_table, cmap='coolwarm', cbar_kws={'label': 'Semantic Distance'})\n",
    "plt.title(\"Semantic Distances Between Classes\")\n",
    "plt.show()\n",
    "\n",
    "# Save the full connection graph to a CSV file\n",
    "class_graph.to_csv(f\"./generated_data/graph/class/{version}_{system}_class_graph.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from cdlib import algorithms\n",
    "from karateclub import EdMot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_communities(type, communities):\n",
    "    for idx, community in enumerate(communities):\n",
    "        print(f\"{type} Service Community {idx + 1}:\")\n",
    "        for class_name in community:\n",
    "            print(f\"  - {class_name}\")\n",
    "        print(\"=\" * 40)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph_reindexed(graph, class_labels_dict, type_label):\n",
    "    classes = [class_name for class_name, label in class_labels_dict.items() if label == type_label]\n",
    "    subgraph = graph.subgraph(classes)\n",
    "    \n",
    "    # Re-index nodes\n",
    "    mapping = {node: i for i, node in enumerate(subgraph.nodes())}\n",
    "    inverse_mapping = {i: node for node, i in mapping.items()}  # This is the correct inverse mapping\n",
    "    subgraph_reindexed = nx.relabel_nodes(subgraph, mapping)\n",
    "    \n",
    "    return subgraph_reindexed, inverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_community_detection(graph, class_labels_dict, type_label, algorithm, level=1, resolution=0.5):\n",
    "    subgraph_reindexed, inverse_mapping = get_subgraph_reindexed(graph, class_labels_dict, type_label)\n",
    "\n",
    "    ALGORITHMS = {\n",
    "        'Louvain': lambda: algorithms.louvain(subgraph_reindexed, weight='weight', resolution=resolution).communities,\n",
    "        'Infomap': lambda: algorithms.infomap(subgraph_reindexed).communities,\n",
    "        'LabelPropagation': lambda: algorithms.label_propagation(subgraph_reindexed).communities,\n",
    "        'GirvanNewman': lambda: algorithms.girvan_newman(subgraph_reindexed, level=level).communities,\n",
    "        'FastGreedy': lambda: algorithms.greedy_modularity(subgraph_reindexed).communities\n",
    "    }\n",
    "    \n",
    "    if len(subgraph_reindexed.nodes()) < 4:\n",
    "        communities_reindexed = [list(community) for community in nx.community.k_clique_communities(subgraph_reindexed, 3)]\n",
    "    else:\n",
    "        if algorithm == 'EdMot':\n",
    "            edmot = EdMot()\n",
    "            edmot.fit(subgraph_reindexed)\n",
    "            memberships = edmot.get_memberships()\n",
    "            unique_communities = set(memberships.values())\n",
    "            communities_reindexed = [list({node for node, community_id in memberships.items() if community_id == c}) for c in unique_communities]\n",
    "        else:\n",
    "            communities_reindexed = ALGORITHMS.get(algorithm, lambda: print(f\"Error: The algorithm '{algorithm}' is not supported. Supported algorithms are: {', '.join(ALGORITHMS.keys())}.\"))()\n",
    "\n",
    "    communities = [[inverse_mapping[node] for node in community] for community in communities_reindexed]\n",
    "\n",
    "    return communities, communities_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NetworkX graph from the class graph\n",
    "G = nx.Graph()\n",
    "for index, row in class_graph.iterrows():\n",
    "    G.add_edge(row['class1'], row['class2'], weight=row['static_distance']) # OR weight=row['combined_distance'] OR weight=row['semantic_distance'] OR weight=row['static_distance']\n",
    "\n",
    "# Specify the algorithm to use (e.g. 'Louvain', 'EdMot', ...)\n",
    "algorithm = 'Louvain'  # Change this\n",
    "\n",
    "# Perform community detection for Application Services using the specified algorithm\n",
    "application_communities, _ = perform_community_detection(G, class_labels_dict, 0, algorithm)\n",
    "print_communities('Application', application_communities)\n",
    "\n",
    "# Perform community detection for Entity Services using the specified algorithm \n",
    "entity_communities, _ = perform_community_detection(G, class_labels_dict, 2, algorithm)\n",
    "print_communities('Entity', entity_communities)\n",
    "\n",
    "# Perform community detection for Utility Services using the specified algorithm\n",
    "utility_communities, _ = perform_community_detection(G, class_labels_dict, 1, algorithm)\n",
    "print_communities('Utility', utility_communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Optimize parameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters based on modularity metric\n",
    "def search_for_best_params_based_on_modularity(graph, class_labels_dict, algorithm):\n",
    "    best_params = {}\n",
    "    total_best_modularity = -1  # initialize with a low value\n",
    "\n",
    "    if algorithm == 'Louvain':\n",
    "        for resolution in np.arange(0.1, 2.0, 0.2):  # example resolution values\n",
    "            total_modularity = 0  # Sum of modularities for all type labels for the current resolution\n",
    "            for type_label in [0, 1, 2]:\n",
    "                subgraph_reindexed, _ = get_subgraph_reindexed(graph, class_labels_dict, type_label)\n",
    "                _, communities_reindexed = perform_community_detection(graph, class_labels_dict, type_label, algorithm, level=None, resolution=resolution)\n",
    "                try:\n",
    "                    modularity_value = nx.community.modularity(subgraph_reindexed, communities_reindexed)\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "                total_modularity += modularity_value  # Add modularity of current type label to the total\n",
    "\n",
    "            if total_modularity > total_best_modularity:  # Check total modularity across all type labels\n",
    "                total_best_modularity = total_modularity\n",
    "                best_params['resolution'] = resolution\n",
    "\n",
    "    elif algorithm == 'GirvanNewman':\n",
    "        for level in range(1, 10):  # example level values\n",
    "            total_modularity = 0  # Sum of modularities for all type labels for the current level\n",
    "            for type_label in [0, 1, 2]:\n",
    "                subgraph_reindexed, _ = get_subgraph_reindexed(graph, class_labels_dict, type_label)\n",
    "                _, communities_reindexed = perform_community_detection(graph, class_labels_dict, type_label, algorithm, level=level)\n",
    "                try:\n",
    "                    modularity_value = nx.community.modularity(subgraph_reindexed, communities_reindexed)\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "                total_modularity += modularity_value  # Add modularity of current type label to the total\n",
    "\n",
    "            if total_modularity > total_best_modularity:  # Check total modularity across all type labels\n",
    "                total_best_modularity = total_modularity\n",
    "                best_params['level'] = level\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search_for_best_params_based_on_modularity(G, class_labels_dict, 'Louvain')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fine-tune clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_cluster(service, services, distance_map):\n",
    "    score_service = {i: 0 for i in range(len(services))}\n",
    "    \n",
    "    for other_service, distance in distance_map.get(service, {}).items():\n",
    "        for i, s in enumerate(services):\n",
    "            if other_service in s:\n",
    "                score_service[i] += distance\n",
    "    \n",
    "    max_score = max(score_service.values())\n",
    "    if max_score > 0:\n",
    "        max_indices = [i for i, x in score_service.items() if x == max_score]\n",
    "        if len(max_indices) == 1:\n",
    "            services[max_indices[0]].append(service)\n",
    "            services = [s for s in services if s != [service]]\n",
    "\n",
    "    return services\n",
    "\n",
    "def fine_tune_all_services(services_list, distances):\n",
    "    distance_map = {s1: {s2: d} for s1, s2, d in distances}\n",
    "    \n",
    "    for i, s in enumerate(services_list):\n",
    "        if len(s) < 2:\n",
    "            services_list = fine_tune_cluster(s[0], services_list, distance_map)\n",
    "    return services_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of distances to be used for fine-tuning\n",
    "distances = [(row['class1'], row['class2'], row['static_distance']) for index, row in class_graph.iterrows()] # OR row['combined_distance'] OR row['semantic_distance'] OR row['static_distance']\n",
    "\n",
    "# Fine-tune the communities using specified distance\n",
    "fine_tuned_application_communities = fine_tune_all_services(application_communities, distances)\n",
    "fine_tuned_entity_communities = fine_tune_all_services(entity_communities, distances)\n",
    "fine_tuned_utility_communities = fine_tune_all_services(utility_communities, distances)\n",
    "\n",
    "print_communities('Application', fine_tuned_application_communities)\n",
    "print_communities('Entity', fine_tuned_entity_communities)\n",
    "print_communities('Utility', fine_tuned_utility_communities)\n",
    "\n",
    "# Save fine-tuned communities in a single CSV file for later use\n",
    "with open(f'generated_data/community/{version}_{system}_{algorithm}_communities.csv', 'w') as f:\n",
    "    f.write('class_name,service\\n')\n",
    "    for i, service in enumerate(fine_tuned_application_communities):\n",
    "        for class_name in service:\n",
    "            f.write(f'{class_name},Application Service {i + 1}\\n')\n",
    "    for i, service in enumerate(fine_tuned_entity_communities):\n",
    "        for class_name in service:\n",
    "            f.write(f'{class_name},Entity Service {i + 1}\\n')\n",
    "    for i, service in enumerate(fine_tuned_utility_communities):\n",
    "        for class_name in service:\n",
    "            f.write(f'{class_name},Utility Service {i + 1}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

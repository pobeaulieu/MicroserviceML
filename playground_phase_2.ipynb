{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Type-based Service Identification (Grouping by Same Type Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "# Pas d'influence du mod√®le d'embeddings puisqu'on n'utilise que les distances statiques\n",
    "model_type = \"codebert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create class graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_class_code_from_directory, load_data_from_csv, print_communities, save_communities_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(system, version, model_type):\n",
    "    \"\"\"Load class code, class names, labels, and embeddings.\"\"\"\n",
    "    class_code = load_class_code_from_directory(system)\n",
    "    class_names, class_labels, class_embeddings = load_data_from_csv(f\"generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\")\n",
    "    return class_code, dict(zip(class_names, class_labels)), dict(zip(class_names, class_embeddings))\n",
    "\n",
    "def normalize_column(df, column_name):\n",
    "    \"\"\"Normalize values of a dataframe column between 0 and 1.\"\"\"\n",
    "    column_min = df[column_name].min()\n",
    "    column_max = df[column_name].max()\n",
    "    df[column_name] = (df[column_name] - column_min) / (column_max - column_min)\n",
    "    return df\n",
    "\n",
    "def load_call_graph(system):\n",
    "    \"\"\"Load the call graph.\"\"\"\n",
    "    file_path = f\"./generated_data/graph/call/{system}_call_graph.csv\"\n",
    "    call_graph = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=';',\n",
    "        header=None,\n",
    "        names=['class1', 'class2', 'static_distance']\n",
    "    )\n",
    "    return call_graph\n",
    "\n",
    "def compute_semantic_distances(embeddings_dict):\n",
    "    \"\"\"Compute pairwise semantic distances.\"\"\"\n",
    "    distances = []\n",
    "    for class_name1, embedding1 in embeddings_dict.items():\n",
    "        for class_name2, embedding2 in embeddings_dict.items():\n",
    "            distance = 1 - spatial.distance.cosine(embedding1, embedding2)\n",
    "            distances.append([class_name1, class_name2, distance])\n",
    "    return pd.DataFrame(distances, columns=['class1', 'class2', 'semantic_distance'])\n",
    "\n",
    "def filter_and_normalize(df, class_labels_dict):\n",
    "    \"\"\"Filter rows based on class labels and normalize specified columns.\"\"\"\n",
    "    valid_class_labels = set(class_labels_dict.keys())\n",
    "    filtered_df = df[df['class1'].isin(valid_class_labels) & df['class2'].isin(valid_class_labels)]\n",
    "    \n",
    "    if 'semantic_distance' in filtered_df.columns:\n",
    "        filtered_df = normalize_column(filtered_df, 'semantic_distance')\n",
    "    if 'static_distance' in filtered_df.columns:\n",
    "        filtered_df = normalize_column(filtered_df, 'static_distance')\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def merge_dataframes(static_df, semantic_df):\n",
    "    \"\"\"Merge structural and semantic dataframes.\"\"\"\n",
    "    merged_df = static_df.merge(semantic_df, on=['class1', 'class2'], how='outer')\n",
    "    return merged_df.fillna({'static_distance': 0, 'semantic_distance': 0})\n",
    "\n",
    "def visualize_heatmap(df, values_column, title):\n",
    "    \"\"\"Visualize a heatmap from a dataframe.\"\"\"\n",
    "    pivot_table = df.pivot(index='class1', columns='class2', values=values_column)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap = sns.heatmap(pivot_table, cmap='coolwarm', cbar_kws={'label': title})\n",
    "    \n",
    "    # Update y-tick labels\n",
    "    ytick_labels = [label.get_text().split('.')[-1] for label in heatmap.get_yticklabels()]\n",
    "    heatmap.set_yticklabels(ytick_labels)\n",
    "    \n",
    "    # Update x-tick labels\n",
    "    xtick_labels = [label.get_text().split('.')[-1] for label in heatmap.get_xticklabels()]\n",
    "    heatmap.set_xticklabels(xtick_labels)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "_, class_labels_dict, class_embeddings_dict = load_data(system, version, model_type)\n",
    "static_df = load_call_graph(system)\n",
    "\n",
    "if system == 'cargotracker': # temporary fix\n",
    "    # Replace 'org.eclipse' with 'net.java' in both class1 and class2 columns\n",
    "    static_df['class1'] = static_df['class1'].str.replace('org.eclipse', 'net.java', regex=False)\n",
    "    static_df['class2'] = static_df['class2'].str.replace('org.eclipse', 'net.java', regex=False)\n",
    "\n",
    "semantic_df = compute_semantic_distances(class_embeddings_dict)\n",
    "static_df = filter_and_normalize(static_df, class_labels_dict)\n",
    "semantic_df = filter_and_normalize(semantic_df, class_labels_dict)\n",
    "class_graph = merge_dataframes(static_df, semantic_df)\n",
    "\n",
    "# Visualizations\n",
    "visualize_heatmap(class_graph, 'static_distance', \"Static Distances Between Classes\")\n",
    "visualize_heatmap(class_graph, 'semantic_distance', \"Semantic Distances Between Classes\")\n",
    "\n",
    "# Save to CSV\n",
    "filename = f\"./generated_data/graph/class/{version}_{system}_class_graph.csv\"\n",
    "class_graph.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from cdlib import algorithms\n",
    "from karateclub import EdMot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityDetection:\n",
    "    \n",
    "    def __init__(self, graph, class_labels_dict, optimize_hyperparameters_flag=False):\n",
    "        self.graph = graph.copy()  # Create a copy of the graph to avoid modifying the original\n",
    "        self.graph.remove_edges_from(nx.selfloop_edges(self.graph))  # Remove self-loops\n",
    "        self.class_labels_dict = class_labels_dict\n",
    "        self.optimize_hyperparameters_flag = optimize_hyperparameters_flag\n",
    "        self.best_params = {}\n",
    "\n",
    "    def get_subgraph_reindexed(self, label_type):\n",
    "        \"\"\"Get a subgraph based on label type and reindex its nodes.\"\"\"\n",
    "        label_mapping = {'Application': 0, 'Utility': 1, 'Entity': 2}\n",
    "        classes = [class_name for class_name, label in self.class_labels_dict.items() if label == label_mapping.get(label_type)]\n",
    "        subgraph = self.graph.subgraph(classes)\n",
    "        self.visualize_subgraph(subgraph, label_type)\n",
    "        mapping = {node: i for i, node in enumerate(subgraph.nodes())}\n",
    "        return nx.relabel_nodes(subgraph, mapping), {i: node for node, i in mapping.items()}\n",
    "\n",
    "    def visualize_subgraph(self, subgraph, label_type):\n",
    "        \"\"\"Visualize the subgraph.\"\"\"\n",
    "        labels = {node: node.split('.')[-1] for node in subgraph.nodes()}\n",
    "        \n",
    "        # Determine edge weights for thickness, and scale for visibility\n",
    "        min_thickness = 0.5  # minimum thickness for edges\n",
    "        max_thickness = 5.0  # maximum thickness for edges\n",
    "        edge_weights = [subgraph[u][v]['static_distance'] for u, v in subgraph.edges()]\n",
    "\n",
    "        if len(edge_weights) == 1:\n",
    "            scaled_weights = [min_thickness + (max_thickness - min_thickness) / 2]  # mid-thickness for a single edge\n",
    "        else:\n",
    "            scaled_weights = [min_thickness + (w - min(edge_weights)) * (max_thickness - min_thickness) / \n",
    "                            (max(edge_weights) - min(edge_weights)) for w in edge_weights]\n",
    "        \n",
    "        # Use spring_layout for node positioning\n",
    "        pos = nx.spring_layout(subgraph, scale=100, weight='static_distance', iterations=100) # change scale to spread nodes more\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        nx.draw(subgraph, pos=pos, with_labels=True, labels=labels, \n",
    "                node_size=500, node_color=\"lightblue\", font_size=8, \n",
    "                font_weight=\"bold\", width=scaled_weights, edge_color=\"grey\")\n",
    "        \n",
    "        plt.title(f\"{label_type} Classes Subgraph\")\n",
    "        plt.show()\n",
    "\n",
    "    def remove_duplicate_single_node_communities(self, communities):\n",
    "        \"\"\"Remove duplicated single-node communities.\"\"\"\n",
    "        # Get the single-node communities\n",
    "        single_node_communities = [community for community in communities if len(community) == 1]\n",
    "        \n",
    "        # Convert to sets for easy comparison and remove duplicates\n",
    "        unique_single_node_communities = set(tuple(community) for community in single_node_communities)\n",
    "        \n",
    "        # Remove all single-node communities from the original list\n",
    "        communities = [community for community in communities if len(community) != 1]\n",
    "        \n",
    "        # Add back the unique single-node communities\n",
    "        communities.extend([list(community) for community in unique_single_node_communities])\n",
    "        \n",
    "        return communities\n",
    "\n",
    "    def _algorithm_edmot(self, subgraph_reindexed):\n",
    "        \"\"\"Compute communities using the EdMot algorithm.\"\"\"\n",
    "        edmot = EdMot()\n",
    "        edmot.fit(subgraph_reindexed)\n",
    "        memberships = edmot.get_memberships()\n",
    "        unique_communities = set(memberships.values())\n",
    "        return [list({node for node, community_id in memberships.items() if community_id == c}) for c in unique_communities]\n",
    "\n",
    "    def detect_communities(self, label_type, algorithm):\n",
    "        \"\"\"Perform community detection based on a specified algorithm.\"\"\"\n",
    "        subgraph_reindexed, inverse_mapping = self.get_subgraph_reindexed(label_type)\n",
    "\n",
    "        # If hyperparameter optimization flag is set, optimize parameters\n",
    "        if self.optimize_hyperparameters_flag:\n",
    "            self.best_params = self.optimize_hyperparameters(subgraph_reindexed, algorithm)\n",
    "            \n",
    "        ALGORITHMS = {\n",
    "            'Louvain': lambda: algorithms.louvain(subgraph_reindexed, weight='weight', resolution=self.best_params.get('resolution', 0.5)).communities,\n",
    "            'Infomap': lambda: algorithms.infomap(subgraph_reindexed).communities,\n",
    "            'LabelPropagation': lambda: algorithms.label_propagation(subgraph_reindexed).communities,\n",
    "            'GirvanNewman': lambda: algorithms.girvan_newman(subgraph_reindexed, level=self.best_params.get('level', 1)).communities,\n",
    "            'FastGreedy': lambda: algorithms.greedy_modularity(subgraph_reindexed).communities,\n",
    "            'EdMot': lambda: self._algorithm_edmot(subgraph_reindexed)\n",
    "        }\n",
    "            \n",
    "        if len(subgraph_reindexed.nodes()) < 4:\n",
    "            communities_reindexed = [list(community) for community in nx.community.k_clique_communities(subgraph_reindexed, 3)]\n",
    "        else:\n",
    "            # If the graph is disconnected and the algorithm is Infomap or FastGreedy, skip the algorithm.\n",
    "            if (not nx.is_connected(subgraph_reindexed) and (algorithm == 'Infomap' or algorithm == 'FastGreedy')):\n",
    "                print(f\"Graph is disconnected. Skipping {algorithm}.\")\n",
    "                return [[inverse_mapping[node]] for node in subgraph_reindexed.nodes()]  # Treat each node as its own community\n",
    "            else:\n",
    "                communities_reindexed = ALGORITHMS.get(algorithm, lambda: print(f\"Error: The algorithm '{algorithm}' is not supported. Supported algorithms are: {', '.join(ALGORITHMS.keys())}.\"))()\n",
    "\n",
    "        # Handle isolated nodes:\n",
    "        # Create separate communities for each of them\n",
    "        isolated_nodes = [node for node in subgraph_reindexed.nodes() if subgraph_reindexed.degree(node) == 0]\n",
    "        for node in isolated_nodes:\n",
    "            communities_reindexed.append([node])\n",
    "\n",
    "        communities = [[inverse_mapping[node] for node in community] for community in communities_reindexed]\n",
    "        \n",
    "        # Remove duplicated single-node communities\n",
    "        communities = self.remove_duplicate_single_node_communities(communities)\n",
    "\n",
    "        return communities\n",
    "\n",
    "    def optimize_hyperparameters(self, label_type, subgraph_reindexed, algorithm):\n",
    "        best_params = {}\n",
    "        best_modularity = -1  # initialize with a low value\n",
    "\n",
    "        PARAM_RANGES = {\n",
    "            'Louvain': (np.arange(0.1, 2.0, 0.2), 'resolution'),\n",
    "            'GirvanNewman': (range(1, 10), 'level')\n",
    "        }\n",
    "\n",
    "        if algorithm in PARAM_RANGES:\n",
    "            param_values, param_name = PARAM_RANGES[algorithm]\n",
    "\n",
    "            for value in param_values:\n",
    "                # Make sure the subgraph is connected before computing modularity\n",
    "                if not nx.is_connected(subgraph_reindexed):\n",
    "                    continue\n",
    "                \n",
    "                communities = self.detect_communities(label_type, algorithm, **{param_name: value})\n",
    "                modularity_value = nx.community.modularity(subgraph_reindexed, communities)\n",
    "\n",
    "                if modularity_value > best_modularity:\n",
    "                    best_modularity = modularity_value\n",
    "                    best_params[param_name] = value\n",
    "        else:\n",
    "            print(f\"Parameter optimization not supported for {algorithm}\")\n",
    "        return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Community fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_cluster(service, services, distance_map):\n",
    "    score_service = {i: 0 for i in range(len(services))}\n",
    "    \n",
    "    for other_service, distance in distance_map.get(service, {}).items():\n",
    "        for i, s in enumerate(services):\n",
    "            if other_service in s:\n",
    "                score_service[i] += distance\n",
    "    \n",
    "    max_score = max(score_service.values())\n",
    "    if max_score > 0:\n",
    "        max_indices = [i for i, x in enumerate(score_service.values()) if x == max_score]\n",
    "        if len(max_indices) == 1:\n",
    "            services[max_indices[0]].append(service)\n",
    "            services = [s for s in services if s != [service]]\n",
    "\n",
    "    return services\n",
    "\n",
    "def fine_tune_all_services(services_list, distances):\n",
    "    distance_map = {s1: {s2: d} for s1, s2, d in distances}\n",
    "    \n",
    "    for i, s in enumerate(services_list):\n",
    "        if len(s) < 2:\n",
    "            services_list = fine_tune_cluster(s[0], services_list, distance_map)\n",
    "    return services_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "G = nx.from_pandas_edgelist(class_graph[class_graph['static_distance'] != 0], 'class1', 'class2', ['static_distance'])\n",
    "cd = CommunityDetection(G, class_labels_dict)  # Set optimize_hyperparameters_flag=True if you wish optimize parameters of clustering algorithms\n",
    "\n",
    "ALGORITHMS = ['Louvain', 'Infomap', 'LabelPropagation', 'GirvanNewman', 'FastGreedy', 'EdMot']\n",
    "\n",
    "# Fine-tuning clusters using static distance\n",
    "distances = [(row['class1'], row['class2'], row['static_distance']) for index, row in class_graph.iterrows()]  # OR other distances\n",
    "\n",
    "for algorithm in ALGORITHMS: # OR use those you need\n",
    "    print(f\"Running {algorithm} algorithm...\")\n",
    "    \n",
    "    communities = {\n",
    "        'Application': cd.detect_communities('Application', algorithm),\n",
    "        'Entity': cd.detect_communities('Entity', algorithm),\n",
    "        'Utility': cd.detect_communities('Utility', algorithm)\n",
    "    }\n",
    "\n",
    "    fine_tuned_communities = {\n",
    "        label_type: fine_tune_all_services(services, distances)\n",
    "        for label_type, services in communities.items()\n",
    "    }\n",
    "\n",
    "    # Print the communities\n",
    "    for label_type, services in fine_tuned_communities.items():\n",
    "        print_communities(label_type, services)\n",
    "\n",
    "    # Save fine-tuned communities to CSV\n",
    "    save_communities_to_csv(fine_tuned_communities, version, system, algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Classification (assign Utility, Application, or Entity Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_team\" # All options: v_imen, v_team\n",
    "systems = [\"cargotracker\", \"pos\", \"jforum\", \"petclinic\"]\n",
    "model_type = \"codebert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert\n",
    "\n",
    "single_system_training = True\n",
    "single_system = \"pos\"\n",
    "\n",
    "training_systems = [\"jforum\", \"cargotracker\", \"petclinic\"]\n",
    "test_systems = [\"pos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AlbertTokenizer, AlbertModel, RobertaModel, RobertaTokenizer, BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils import load_class_code_from_directory, load_data_from_csv, write_embeddings_to_csv, associate_classes_to_types\n",
    "from embeddings import generate_embeddings_for_java_code, generate_word_embeddings_for_java_code\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU) is available and if so, set the device to GPU\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and tokenizer\n",
    "if (model_type == \"codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "elif (model_type == \"ft_codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "elif (model_type == \"bert\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "elif (model_type == \"roberta\"):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "elif (model_type == \"albert\"): \n",
    "    # pip3 install sentencepiece\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "    model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "elif model_type == \"word2vec\":\n",
    "    # Download required NLTK datasets and initialize the lemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Load Word2Vec model\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "else:\n",
    "    raise NameError(\"model type not supported\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_system_training:\n",
    "    # Labels are 0: Application, 1: Utility, 2: Entity\n",
    "    class_labels = associate_classes_to_types(version, single_system)\n",
    "    print(class_labels)\n",
    "    # For each class in class_code, generate embeddings and add to class_embeddings dictionary\n",
    "    class_embeddings = {}\n",
    "    class_code = load_class_code_from_directory(single_system)\n",
    "    if model_type == \"word2vec\":\n",
    "        class_embeddings = {class_name: generate_word_embeddings_for_java_code(code, word2vec_model, word_lemmatizer) for class_name, code in class_code.items()}\n",
    "    else:\n",
    "        class_embeddings = {class_name: generate_embeddings_for_java_code(code, model, tokenizer, device) for class_name, code in class_code.items()}\n",
    "\n",
    "    # Write embeddings to csv file\n",
    "    write_embeddings_to_csv(version, single_system, model_type, class_embeddings, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not single_system_training:\n",
    "    for system in systems:\n",
    "        # Labels are 0: Application, 1: Utility, 2: Entity\n",
    "        class_labels = associate_classes_to_types(version, system)\n",
    "        print(f\"Processing system: {system}, number of classes: {len(class_labels)}\")\n",
    "\n",
    "        # For each class in class_code, generate embeddings and add to class_embeddings dictionary\n",
    "        class_embeddings = {}\n",
    "        class_code = load_class_code_from_directory(system)\n",
    "        if model_type == \"word2vec\":\n",
    "            class_embeddings = {class_name: generate_word_embeddings_for_java_code(code, word2vec_model, word_lemmatizer) for class_name, code in class_code.items()}\n",
    "        else:\n",
    "            class_embeddings = {class_name: generate_embeddings_for_java_code(code, model, tokenizer, device) for class_name, code in class_code.items()}\n",
    "\n",
    "        # Write embeddings to csv file\n",
    "        write_embeddings_to_csv(version, system, model_type, class_embeddings, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple systems data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not single_system_training:\n",
    "    training_embeddings = []\n",
    "    training_labels = []\n",
    "    training_class_names = []\n",
    "\n",
    "    for system in training_systems:\n",
    "        # Load data\n",
    "        filename = f\"./generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\"\n",
    "        class_names, labels, embeddings = load_data_from_csv(filename)\n",
    "\n",
    "        training_embeddings.extend(embeddings)\n",
    "        training_labels.extend(labels)\n",
    "        training_class_names.extend(class_names)\n",
    "\n",
    "    # We use the combined arrays, no split anymore\n",
    "    Xtrain = np.array(training_embeddings)\n",
    "    ytrain = np.array(training_labels)\n",
    "\n",
    "    # Ensure at least one instance of each class in the training data\n",
    "    unique_classes = set(labels)\n",
    "    for cls in unique_classes:\n",
    "        if cls not in ytrain:\n",
    "            cls_index = labels.index(cls)\n",
    "            Xtrain = np.vstack([Xtrain, [embeddings[cls_index]]])\n",
    "            ytrain.append(cls)\n",
    "\n",
    "    # Calculate class frequencies and mean frequency\n",
    "    class_freq = Counter(ytrain)\n",
    "    mean_count = sum(class_freq.values()) // len(class_freq)\n",
    "\n",
    "    # Identify classes that need resampling (e.g., significantly fewer than mean_count)\n",
    "    threshold = 0.7  # 70% of the mean_count\n",
    "    classes_to_resample = {cls: int(mean_count) for cls, count in class_freq.items() if count < mean_count * threshold}\n",
    "\n",
    "    # Check if all classes_to_resample have enough samples\n",
    "    skip_resampling = False\n",
    "    for class_label in classes_to_resample.keys():\n",
    "        if class_label in class_freq and class_freq[class_label] < 2:  # k_neighbors + 1\n",
    "            skip_resampling = True\n",
    "            print(f\"Skipping resampling for class {class_label} due to insufficient samples.\")\n",
    "            break\n",
    "\n",
    "    # Apply SMOTE\n",
    "    if not skip_resampling and classes_to_resample:\n",
    "        sm = SMOTE(sampling_strategy=classes_to_resample, k_neighbors=1, random_state=42)\n",
    "        Xtrain, ytrain = sm.fit_resample(Xtrain, ytrain)\n",
    "\n",
    "    print(f'Number of classes: {len(training_class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not single_system_training:\n",
    "    test_embeddings = []\n",
    "    test_labels = []\n",
    "    test_class_names = []\n",
    "\n",
    "    for system in test_systems:\n",
    "        # Load data\n",
    "        filename = f\"./generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\"\n",
    "        class_names, labels, embeddings = load_data_from_csv(filename)\n",
    "\n",
    "        test_embeddings.extend(embeddings)\n",
    "        test_labels.extend(labels)\n",
    "        test_class_names.extend(class_names)\n",
    "\n",
    "    Xtest = np.array(test_embeddings)\n",
    "    ytest = np.array(test_labels)\n",
    "\n",
    "    print(f'Number of classes: {len(test_class_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single system data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_system_training:\n",
    "    # Load data\n",
    "    filename = f\"./generated_data/embedding/{version}_{single_system}_{model_type}_embeddings.csv\"\n",
    "    class_names, labels, embeddings = load_data_from_csv(filename)\n",
    "\n",
    "    # Train-test split\n",
    "    Xtrain, Xtest, names_train, names_test = train_test_split(embeddings, class_names, test_size=0.3, random_state=0)\n",
    "\n",
    "    # Get labels for the training and test sets\n",
    "    ytrain, ytest = [[labels[class_names.index(name)] for name in lst] for lst in [names_train, names_test]]\n",
    "\n",
    "    # Ensure at least one instance of each class in the training data\n",
    "    unique_classes = set(labels)\n",
    "    for cls in unique_classes:\n",
    "        if cls not in ytrain:\n",
    "            cls_index = labels.index(cls)\n",
    "            Xtrain = np.vstack([Xtrain, [embeddings[cls_index]]])\n",
    "            ytrain.append(cls)\n",
    "\n",
    "    # Calculate class frequencies and mean frequency\n",
    "    class_freq = Counter(ytrain)\n",
    "    mean_count = sum(class_freq.values()) // len(class_freq)\n",
    "\n",
    "    # Identify classes that need resampling (e.g., significantly fewer than mean_count)\n",
    "    threshold = 0.7  # 70% of the mean_count\n",
    "    classes_to_resample = {cls: int(mean_count) for cls, count in class_freq.items() if count < mean_count * threshold}\n",
    "\n",
    "    # Check if all classes_to_resample have enough samples\n",
    "    skip_resampling = False\n",
    "    for class_label in classes_to_resample.keys():\n",
    "        if class_label in class_freq and class_freq[class_label] < 2:  # k_neighbors + 1\n",
    "            skip_resampling = True\n",
    "            print(f\"Skipping resampling for class {class_label} due to insufficient samples.\")\n",
    "            break\n",
    "\n",
    "    # Apply SMOTE\n",
    "    if not skip_resampling and classes_to_resample:\n",
    "        sm = SMOTE(sampling_strategy=classes_to_resample, k_neighbors=1, random_state=42)\n",
    "        Xtrain, ytrain = sm.fit_resample(Xtrain, ytrain)\n",
    "\n",
    "    # Calculate total type repartition\n",
    "\n",
    "    # # Print information\n",
    "    # print(f'Number of classes: {len(class_names)}\\nResampled dataset shape: {Counter(ytrain)}')\n",
    "    # print(f'len(Xtrain): {len(Xtrain)}\\nlen(Xtest): {len(Xtest)}')\n",
    "    # print(f'ytrain: {ytrain}\\nytest: {ytest}')\n",
    "    # print(f'ytrain count: {np.bincount(ytrain)}\\nytest count: {np.bincount(ytest)}')\n",
    "    # print(f'total class types repartition : {np.bincount(ytrain) + np.bincount(ytest)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information\n",
    "print(f'Resampled dataset shape: {Counter(ytrain)}')\n",
    "print(f'len(Xtrain): {len(Xtrain)}\\nlen(Xtest): {len(Xtest)}')\n",
    "print(f'ytrain: {ytrain}\\nytest: {ytest}')\n",
    "print(f'ytrain count: {np.bincount(ytrain)}\\nytest count: {np.bincount(ytest)}')\n",
    "print(f'total class types repartition : {np.bincount(ytrain) + np.bincount(ytest)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_system_training:\n",
    "    # OPTIONAL: Please keep this code commented out unless you want to use it\n",
    "    # This code ensures that each class is represented in the test set, proportional to its representation in the training set\n",
    "    # Calculate class frequencies for training and test sets\n",
    "    train_class_freq = Counter(ytrain)\n",
    "    test_class_freq = Counter(ytest)\n",
    "\n",
    "    # Calculate the ratio between the size of the training and test sets\n",
    "    ratio = len(ytrain) / len(ytest)\n",
    "\n",
    "    # Iterate over each class to make sure it has proportional representation in the test set\n",
    "    for cls, train_count in train_class_freq.items():\n",
    "        expected_test_count = int(train_count / ratio)\n",
    "        actual_test_count = test_class_freq.get(cls, 0)\n",
    "\n",
    "        if actual_test_count < expected_test_count:\n",
    "            # Find instances in the training set to move to the test set\n",
    "            for _ in range(expected_test_count - actual_test_count):\n",
    "                cls_index = ytrain.index(cls)\n",
    "                Xtest = np.vstack([Xtest, [Xtrain[cls_index]]])\n",
    "                ytest.append(cls)\n",
    "                Xtrain = np.delete(Xtrain, cls_index, axis=0)\n",
    "                ytrain.pop(cls_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des rapports de classification (version textuelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(y_true, y_pred):\n",
    "    # Identify unique labels in both true labels and predictions\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    dynamic_label_names = [label_names_map[label] for label in unique_labels]\n",
    "\n",
    "    # Generate and print the classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=dynamic_label_names, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des fichiers CSV pour la classification de chaque modèle et modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_to_csv(y_true, y_pred, model_name, embedding_model):\n",
    "\n",
    "    csv_file = f'generated_data/visualization/classification_reports_{embedding_model}.csv'\n",
    "\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    dynamic_label_names = [label_names_map[label] for label in unique_labels]\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=dynamic_label_names, output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Create a DataFrame from the report\n",
    "    new_data = pd.DataFrame(report).transpose().reset_index()\n",
    "    new_data.columns = ['label', 'precision', 'recall', 'f1-score', 'support']\n",
    "    new_data['model_name'] = model_name\n",
    "    \n",
    "    # If CSV file exists and is non-empty, load it and filter out old model data\n",
    "    if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n",
    "        existing_data = pd.read_csv(csv_file)\n",
    "        # Filter out the old data for the current model\n",
    "        existing_data = existing_data[existing_data['model_name'] != model_name]\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Concatenate new data with existing data\n",
    "    combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to CSV\n",
    "    combined_data.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class types repartition representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_counts(ytrain_count, ytest_count):\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    labels = [label_names_map[i] for i in range(len(ytrain_count))]\n",
    "\n",
    "    # Colors for each bar\n",
    "    colors_train = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "    colors_test = ['blue', 'green', 'red']\n",
    "\n",
    "    # Generate the histogram\n",
    "    plt.bar(labels, ytrain_count, color=colors_train, label='ytrain')\n",
    "    plt.bar(labels, ytest_count, bottom=ytrain_count, color=colors_test, label='ytest')\n",
    "    \n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Total type repartition')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_counts(np.bincount(ytrain), np.bincount(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération d'histogramme pour la répartition des élèments par classe par modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_for_types(y_true, y_pred, prediction_model_name):\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
    "\n",
    "    # Extract support values\n",
    "    supports = [report[str(key)]['support'] for key in sorted(report.keys())[:-3]]\n",
    "    labels = [label_names_map[int(key)] for key in sorted(report.keys())[:-3]]\n",
    "\n",
    "    # Colors for each bar\n",
    "    colors = ['blue', 'green', 'red']\n",
    "\n",
    "    # Generate the histogram\n",
    "    plt.bar(labels, supports, color=colors)\n",
    "    plt.ylabel('Number of classes')\n",
    "    plt.title(f'Type repartition using {prediction_model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=2).fit(Xtrain, ytrain)\n",
    "decision_tree_predictions = decision_tree_classifier.predict(Xtest)\n",
    "decision_tree_accuracy = accuracy_score(ytest, decision_tree_predictions)\n",
    "decision_tree_confusion_matrix = confusion_matrix(ytest, decision_tree_predictions)\n",
    "print(decision_tree_accuracy)\n",
    "print(decision_tree_confusion_matrix)\n",
    "generate_classification_report(ytest, decision_tree_predictions)\n",
    "generate_classification_report_for_types(ytest, decision_tree_predictions, \"Decision Tree\")\n",
    "generate_classification_report_to_csv(ytest, decision_tree_predictions, \"decision_tree\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='linear', C=2, probability=True).fit(Xtrain, ytrain)\n",
    "svm_predictions = svm_classifier.predict(Xtest)\n",
    "svm_accuracy = accuracy_score(ytest, svm_predictions)\n",
    "svm_confusion_matrix = confusion_matrix(ytest, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(svm_confusion_matrix)\n",
    "generate_classification_report(ytest, svm_predictions)\n",
    "generate_classification_report_for_types(ytest, svm_predictions, \"SVM\")\n",
    "generate_classification_report_to_csv(ytest, svm_predictions, \"svm\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)\n",
    "knn_predictions = knn_classifier.predict(Xtest)\n",
    "knn_accuracy = accuracy_score(ytest, knn_predictions)\n",
    "knn_confusion_matrix = confusion_matrix(ytest, knn_predictions)\n",
    "print(knn_accuracy)\n",
    "print(knn_confusion_matrix)\n",
    "generate_classification_report(ytest, knn_predictions)\n",
    "generate_classification_report_for_types(ytest, knn_predictions, \"KNN\")\n",
    "generate_classification_report_to_csv(ytest, knn_predictions, \"knn\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier = LogisticRegression(random_state=0).fit(Xtrain, ytrain)\n",
    "logistic_regression_predictions = logistic_regression_classifier.predict(Xtest)\n",
    "logistic_regression_accuracy = accuracy_score(ytest, logistic_regression_predictions)\n",
    "logistic_regression_confusion_matrix = confusion_matrix(ytest, logistic_regression_predictions)\n",
    "print(logistic_regression_accuracy)\n",
    "print(logistic_regression_confusion_matrix)\n",
    "generate_classification_report(ytest, logistic_regression_predictions)\n",
    "generate_classification_report_for_types(ytest, logistic_regression_predictions, \"Logistic Regression\")\n",
    "generate_classification_report_to_csv(ytest, logistic_regression_predictions, \"logistic_regression\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = GaussianNB().fit(Xtrain, ytrain)\n",
    "naive_bayes_predictions = naive_bayes_classifier.predict(Xtest)\n",
    "naive_bayes_accuracy = accuracy_score(ytest, naive_bayes_predictions)\n",
    "naive_bayes_confusion_matrix = confusion_matrix(ytest, naive_bayes_predictions)\n",
    "print(naive_bayes_accuracy)\n",
    "print(naive_bayes_confusion_matrix)\n",
    "generate_classification_report(ytest, naive_bayes_predictions)\n",
    "generate_classification_report_for_types(ytest, naive_bayes_predictions, \"Gaussian NB\")\n",
    "generate_classification_report_to_csv(ytest, naive_bayes_predictions, \"gaussian_nb\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual classifiers into an ensemble\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "('svm', svm_classifier), ('knn', knn_classifier), ('dt', decision_tree_classifier), ('log_reg', logistic_regression_classifier), ('gnb', naive_bayes_classifier)],\n",
    "voting='soft')\n",
    "\n",
    "# Use SVM as the base estimator for AdaBoost\n",
    "svm_base = SVC(kernel='linear', C=2, probability=True) # Can use any other classifier as the base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=svm_base, n_estimators=50, algorithm='SAMME.R', random_state=1)\n",
    "\n",
    "# Combine the ensemble classifier with AdaBoost\n",
    "final_ensemble = VotingClassifier(estimators=[\n",
    "    ('ensemble_clf', ensemble_clf), ('ada_boost', ada_boost)],\n",
    "    voting='soft')\n",
    "\n",
    "# Fit model to your data\n",
    "final_ensemble.fit(Xtrain, ytrain)\n",
    "\n",
    "# Evaluate model\n",
    "ensemble_predictions = final_ensemble.predict(Xtest)\n",
    "ensemble_accuracy = accuracy_score(ytest, ensemble_predictions)\n",
    "print('Accuracy:', ensemble_accuracy)\n",
    "ensemble_confusion_matrix = confusion_matrix(ytest, ensemble_predictions)\n",
    "print(ensemble_confusion_matrix)\n",
    "generate_classification_report(ytest, ensemble_predictions)\n",
    "generate_classification_report_for_types(ytest, ensemble_predictions, \"Ensemble learning\")\n",
    "generate_classification_report_to_csv(ytest, ensemble_predictions, \"ensemble_learning\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract average value for all models and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_avg_metrics_and_save(embedding_models):\n",
    "    # Placeholder DataFrame to aggregate results\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    \n",
    "    for embedding_model in embedding_models:\n",
    "        # Construct the file path based on the provided embedding model name\n",
    "        file_path = f\"generated_data/visualization/classification_reports_{embedding_model}.csv\"\n",
    "        \n",
    "        # Read the CSV data into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter rows where label is 'macro avg'\n",
    "        macro_avg_df = df[df['label'] == 'macro avg']\n",
    "\n",
    "        # Extract desired columns, values, and add embedding model name\n",
    "        macro_avg_df['embedding_model'] = embedding_model\n",
    "        result = macro_avg_df[['embedding_model', 'model_name', 'precision', 'recall', 'f1-score']]\n",
    "        \n",
    "        # Append the result to the aggregated dataframe\n",
    "        aggregated_df = pd.concat([aggregated_df, result], ignore_index=True)\n",
    "\n",
    "    # Save the aggregated dataframe to a new CSV\n",
    "    aggregated_df.to_csv(\"generated_data/visualization/aggregated_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_list = [\"codebert\", \"albert\", \"bert\", \"roberta\", \"ft_codebert\", \"word2vec\"]  # add other embedding model names as needed\n",
    "extract_avg_metrics_and_save(embedding_models_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

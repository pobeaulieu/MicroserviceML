{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Classification (assign Utility, Application, or Entity Tag)\n",
    "\n",
    "## 1.1 Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_team\" # TODO changer\n",
    "system = \"pos\" # TODO changer\n",
    "model_type = \"albert\" # or ft_codebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AlbertTokenizer, AlbertModel, RobertaModel, RobertaTokenizer, BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import load_class_code_from_directory, load_data_from_csv, save_embeddings_to_csv, process_files\n",
    "from embeddings import generate_embeddings_for_java_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU) is available and if so, set the device to GPU\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and tokenizer\n",
    "if (model_type == \"codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "elif (model_type == \"ft_codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "elif (model_type == \"hugging-face\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "elif (model_type == \"roberta\"):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "elif (model_type == \"albert\"): \n",
    "    # pip3 install sentencepiece\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "    model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "else:\n",
    "    raise NameError(\"model type not supported\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'net.jforum.actions.extensions.TopicWatchExtension': 0, 'net.jforum.actions.interceptors.ControllerSecurityInterceptor': 0, 'net.jforum.actions.interceptors.ExtendsAnnotationInterceptor': 0, 'net.jforum.actions.interceptors.MethodSecurityInterceptor': 0, 'net.jforum.actions.interceptors.SecurityInterceptor': 0, 'net.jforum.actions.interceptors.SessionManagerInterceptor': 0, 'net.jforum.api.JForumExecutionContext': 0, 'net.jforum.controllers.AvatarAdminController': 0, 'net.jforum.controllers.BadWordAdminController': 0, 'net.jforum.controllers.BanlistAdminController': 0, 'net.jforum.controllers.CategoryAdminController': 0, 'net.jforum.controllers.ConfigController': 0, 'net.jforum.controllers.ForumAdminController': 0, 'net.jforum.controllers.ForumController': 0, 'net.jforum.controllers.GroupAdminController': 0, 'net.jforum.controllers.HibernateStatisticsController': 0, 'net.jforum.controllers.LuceneAdminController': 0, 'net.jforum.controllers.MessageController': 0, 'net.jforum.controllers.ModerationController': 0, 'net.jforum.controllers.PostController': 0, 'net.jforum.controllers.PostReportController': 0, 'net.jforum.controllers.PrivateMessageController': 0, 'net.jforum.controllers.RSSController': 0, 'net.jforum.controllers.RankingAdminController': 0, 'net.jforum.controllers.RecentTopicsController': 0, 'net.jforum.controllers.SearchController': 0, 'net.jforum.controllers.SmilieAdminController': 0, 'net.jforum.controllers.ThemeController': 0, 'net.jforum.controllers.TopicController': 0, 'net.jforum.core.events.post.BadWordEvent': 0, 'net.jforum.core.events.post.ForumPostEvent': 0, 'net.jforum.core.events.post.TopicPostEvent': 0, 'net.jforum.core.events.post.TopicReplyEvent': 0, 'net.jforum.core.events.post.TopicWatchPostEvent': 0, 'net.jforum.core.events.topic.ForumTopicEvent': 0, 'net.jforum.core.events.topic.TopicWatchTopicEvent': 0, 'net.jforum.core.support.hibernate.CacheEvictionRules': 0, 'net.jforum.core.support.hibernate.SessionFactoryCreator': 0, 'net.jforum.core.support.hibernate.SpringInterceptor': 0, 'net.jforum.core.support.hibernate.SpringSessionFactory': 0, 'net.jforum.core.SessionManager': 0, 'net.jforum.core.UserSessionListener': 0, 'net.jforum.core.VRaptorCustomPathResolver': 0, 'net.jforum.core.VRaptorSpringProvider': 0, 'net.jforum.events.listeners.AbstractListener': 0, 'net.jforum.events.listeners.CategoryEventListener': 0, 'net.jforum.events.listeners.ForumEventListener': 0, 'net.jforum.events.listeners.PostEventListener': 0, 'net.jforum.events.listeners.TopicEventListener': 0, 'net.jforum.events.EmptyCategoryEvent': 0, 'net.jforum.events.EmptyForumEvent': 0, 'net.jforum.events.EmptyPostEvent': 0, 'net.jforum.events.EmptyTopicEvent': 0, 'net.jforum.events.Event': 0, 'net.jforum.extensions.ActionExtension': 0, 'net.jforum.extensions.ActionExtensionManager': 0, 'net.jforum.extensions.Extends': 0, 'net.jforum.extensions.PostReportCounterOperation': 0, 'net.jforum.extensions.RequestOperation': 0, 'net.jforum.extensions.RequestOperationChain': 0, 'net.jforum.plugins.post.ForumAdminExtension': 0, 'net.jforum.plugins.post.PostEditInterceptor': 0, 'net.jforum.security.AccessForumRule': 0, 'net.jforum.security.AccessRule': 0, 'net.jforum.security.AdministrationRule': 0, 'net.jforum.security.AuthenticatedRule': 0, 'net.jforum.security.ChangePostRule': 0, 'net.jforum.security.CreateNewTopicRule': 0, 'net.jforum.security.DownloadAttachmentRule': 0, 'net.jforum.security.EditUserRule': 0, 'net.jforum.security.EmptyRule': 0, 'net.jforum.security.ModerationRule': 0, 'net.jforum.security.PrivateMessageEnabledRule': 0, 'net.jforum.security.PrivateMessageOwnerRule': 0, 'net.jforum.security.ReplyTopicRule': 0, 'net.jforum.security.RoleManager': 0, 'net.jforum.security.TopicFilter': 0, 'net.jforum.services.AttachmentService': 0, 'net.jforum.services.AvatarService': 0, 'net.jforum.services.CategoryService': 0, 'net.jforum.services.ConfigService': 0, 'net.jforum.services.ForumService': 0, 'net.jforum.services.GroupService': 0, 'net.jforum.services.LostPasswordService': 0, 'net.jforum.services.MessageFormatService': 0, 'net.jforum.services.ModerationLogService': 0, 'net.jforum.services.ModerationService': 0, 'net.jforum.services.MostUsersEverOnlineService': 0, 'net.jforum.services.PollService': 0, 'net.jforum.services.PostService': 0, 'net.jforum.services.PrivateMessageService': 0, 'net.jforum.services.RSSService': 0, 'net.jforum.services.RankingService': 0, 'net.jforum.services.SmilieService': 0, 'net.jforum.services.TopicService': 0, 'net.jforum.services.TopicWatchService': 0, 'net.jforum.services.UserService': 0, '': 0, 'net.jforum.actions.helpers.ActionUtils': 1, 'net.jforum.actions.helpers.Actions': 1, 'net.jforum.actions.helpers.ApproveInfo': 1, 'net.jforum.actions.helpers.AttachedFile': 1, 'net.jforum.actions.helpers.Domain': 1, 'net.jforum.actions.helpers.PostFormOptions': 1, 'net.jforum.controllers.UserAdminController': 1, 'net.jforum.controllers.UserController': 1, 'net.jforum.controllers.AdminController': 1, 'net.jforum.core.events.category.NewCategoryGroupPermissionsEvent': 1, 'net.jforum.core.events.forum.NewForumGroupPermissionsEvent': 1, 'net.jforum.core.exceptions.AccessRuleException': 1, 'net.jforum.core.exceptions.DatabaseException': 1, 'net.jforum.core.exceptions.ForumException': 1, 'net.jforum.core.exceptions.MailException': 1, 'net.jforum.core.exceptions.ValidationException': 1, 'net.jforum.core.support.hibernate.QueryCache': 1, 'net.jforum.core.support.hibernate.QueryCacheFactory': 1, 'net.jforum.core.support.spring.HttpServletRequestFactoryBean': 1, 'net.jforum.core.support.spring.HttpServletResponseFactoryBean': 1, 'net.jforum.core.support.spring.RoleManagerFactoryBean': 1, 'net.jforum.core.tags.DisplayCategoriesTag': 1, 'net.jforum.core.tags.DisplayFormattedMessageTag': 1, 'net.jforum.core.tags.DisplayForumsTag': 1, 'net.jforum.core.tags.FormatSignatureTag': 1, 'net.jforum.core.tags.Functions': 1, 'net.jforum.core.tags.I18nTag': 1, 'net.jforum.core.tags.ImportFileTag': 1, 'net.jforum.core.tags.ImportResponseWrapper': 1, 'net.jforum.core.tags.JForumTag': 1, 'net.jforum.core.tags.SettingsTag': 1, 'net.jforum.core.tags.TemplateResourceTag': 1, 'net.jforum.core.tags.URLTag': 1, 'net.jforum.core.PageExtensionRedirectFilter': 1, 'net.jforum.core.Role': 1, 'net.jforum.core.SecurityConstraint': 1, 'net.jforum.core.UrlPattern': 1, 'net.jforum.formatters.BBCode': 1, 'net.jforum.formatters.BBConfigFormatter': 1, 'net.jforum.formatters.Formatter': 1, 'net.jforum.formatters.HtmlEntitiesFormatter': 1, 'net.jforum.formatters.NewLineToHtmlBreakFormatter': 1, 'net.jforum.formatters.PostFormatters': 1, 'net.jforum.formatters.PostOptions': 1, 'net.jforum.formatters.SafeHtmlAttributesFormatter': 1, 'net.jforum.formatters.SafeHtmlTagsFormatter': 1, 'net.jforum.formatters.SmiliesFormatter': 1, 'net.jforum.sso.DefaultLoginAuthenticator': 1, 'net.jforum.sso.LoginAuthenticator': 1, 'net.jforum.sso.RemoteUserSSO': 1, 'net.jforum.sso.SSO': 1, 'net.jforum.sso.SSOUtils': 1, 'net.jforum.util.CategoryOrderComparator': 1, 'net.jforum.util.ConfigKeys': 1, 'net.jforum.util.GroupInteractionFilter': 1, 'net.jforum.util.I18n': 1, 'net.jforum.util.ImageInfo': 1, 'net.jforum.util.ImageUtils': 1, 'net.jforum.util.JForumConfig': 1, 'net.jforum.util.MD5': 1, 'net.jforum.util.SafeHtml': 1, 'net.jforum.util.SecurityConstants': 1, 'net.jforum.util.URLBuilder': 1, 'net.jforum.util.UploadUtils': 1, 'net.jforum.util.mail.ActivationKeySpammer': 1, 'net.jforum.util.mail.EmailSenderTask': 1, 'net.jforum.util.mail.IdentifiableMimeMessage': 1, 'net.jforum.util.mail.LostPasswordSpammer': 1, 'net.jforum.util.mail.MessageId': 1, 'net.jforum.util.mail.PrivateMessageSpammer': 1, 'net.jforum.util.mail.Spammer': 1, 'net.jforum.util.mail.SpammerFactory': 1, 'net.jforum.util.mail.SpammerTaskExecutor': 1, 'net.jforum.util.mail.TopicReplySpammer': 1, 'net.jforum.entities.util.PaginatedResult': 2, 'net.jforum.entities.util.Pagination': 2, 'net.jforum.entities.util.SearchMatchType': 2, 'net.jforum.entities.util.SearchParams': 2, 'net.jforum.entities.util.SearchResult': 2, 'net.jforum.entities.util.SearchSort': 2, 'net.jforum.entities.util.SearchSortType': 2, 'net.jforum.entities.Attachment': 2, 'net.jforum.entities.AttachmentExtension': 2, 'net.jforum.entities.AttachmentExtensionGroup': 2, 'net.jforum.entities.AttachmentQuota': 2, 'net.jforum.entities.Avatar': 2, 'net.jforum.entities.AvatarType': 2, 'net.jforum.entities.BadWord': 2, 'net.jforum.entities.Banlist': 2, 'net.jforum.entities.Category': 2, 'net.jforum.entities.Config': 2, 'net.jforum.entities.Forum': 2, 'net.jforum.entities.ForumStats': 2, 'net.jforum.entities.ForumWatch': 2, 'net.jforum.entities.Group': 2, 'net.jforum.entities.ModerationLog': 2, 'net.jforum.entities.ModerationLogType': 2, 'net.jforum.entities.MostUsersEverOnline': 2, 'net.jforum.entities.Poll': 2, 'net.jforum.entities.PollOption': 2, 'net.jforum.entities.PollVoter': 2, 'net.jforum.entities.Post': 2, 'net.jforum.entities.PostReport': 2, 'net.jforum.entities.PostReportStatus': 2, 'net.jforum.entities.PrivateMessage': 2, 'net.jforum.entities.PrivateMessageType': 2, 'net.jforum.entities.Ranking': 2, 'net.jforum.entities.Role': 2, 'net.jforum.entities.Session': 2, 'net.jforum.entities.Smilie': 2, 'net.jforum.entities.Theme': 2, 'net.jforum.entities.Topic': 2, 'net.jforum.entities.TopicWatch': 2, 'net.jforum.entities.User': 2, 'net.jforum.entities.UserSession': 2, 'net.jforum.plugins.post.ForumLimitedTime': 2, 'net.jforum.plugins.post.ForumLimitedTimeRepository': 2, 'net.jforum.repository.AvatarRepository': 2, 'net.jforum.repository.BadWordRepository': 2, 'net.jforum.repository.BanlistRepository': 2, 'net.jforum.repository.CategoryRepository': 2, 'net.jforum.repository.ConfigRepository': 2, 'net.jforum.repository.ForumRepository': 2, 'net.jforum.repository.GroupRepository': 2, 'net.jforum.repository.HibernateGenericDAO': 2, 'net.jforum.repository.ModerationLogRepository': 2, 'net.jforum.repository.PollRepository': 2, 'net.jforum.repository.PostReportRepository': 2, 'net.jforum.repository.PostRepository': 2, 'net.jforum.repository.PrivateMessageRepository': 2, 'net.jforum.repository.RSSRepository': 2, 'net.jforum.repository.RankingRepository': 2, 'net.jforum.repository.RecentTopicsRepository': 2, 'net.jforum.repository.Repository': 2, 'net.jforum.repository.SearchRepository': 2, 'net.jforum.repository.SessionRepository': 2, 'net.jforum.repository.SmilieRepository': 2, 'net.jforum.repository.ThemeRepository': 2, 'net.jforum.repository.TopicRepository': 2, 'net.jforum.repository.TopicWatchRepository': 2, 'net.jforum.repository.UserRepository': 2}\n"
     ]
    }
   ],
   "source": [
    "# Labels are 0: Application, 1: Utility, 2: Entity\n",
    "class_labels = process_files(version, system)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m class_code \u001b[39m=\u001b[39m load_class_code_from_directory(system)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m class_name, code \u001b[39min\u001b[39;00m class_code\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     class_embeddings[class_name] \u001b[39m=\u001b[39m generate_embeddings_for_java_file(code, model, tokenizer, device)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Write embeddings to csv file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m save_embeddings_to_csv(version, system, model_type, class_embeddings, class_labels)\n",
      "File \u001b[0;32m~/Documents/PFE/MicroserviceML/embeddings.py:29\u001b[0m, in \u001b[0;36mgenerate_embeddings_for_java_file\u001b[0;34m(code, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Generate embeddings using the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Retrieve the [CLS] token's embeddings (the first token) from the outputs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m cls_embedding \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/albert/modeling_albert.py:732\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    727\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    729\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    730\u001b[0m     input_ids, position_ids\u001b[39m=\u001b[39mposition_ids, token_type_ids\u001b[39m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds\n\u001b[1;32m    731\u001b[0m )\n\u001b[0;32m--> 732\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    733\u001b[0m     embedding_output,\n\u001b[1;32m    734\u001b[0m     extended_attention_mask,\n\u001b[1;32m    735\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    736\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    737\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    738\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    739\u001b[0m )\n\u001b[1;32m    741\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    743\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler_activation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output[:, \u001b[39m0\u001b[39m])) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/albert/modeling_albert.py:481\u001b[0m, in \u001b[0;36mAlbertTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39m# Index of the hidden group\u001b[39;00m\n\u001b[1;32m    479\u001b[0m group_idx \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(i \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_groups))\n\u001b[0;32m--> 481\u001b[0m layer_group_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malbert_layer_groups[group_idx](\n\u001b[1;32m    482\u001b[0m     hidden_states,\n\u001b[1;32m    483\u001b[0m     attention_mask,\n\u001b[1;32m    484\u001b[0m     head_mask[group_idx \u001b[39m*\u001b[39;49m layers_per_group : (group_idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m layers_per_group],\n\u001b[1;32m    485\u001b[0m     output_attentions,\n\u001b[1;32m    486\u001b[0m     output_hidden_states,\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    488\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_group_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/albert/modeling_albert.py:433\u001b[0m, in \u001b[0;36mAlbertLayerGroup.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    430\u001b[0m layer_attentions \u001b[39m=\u001b[39m ()\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m layer_index, albert_layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malbert_layers):\n\u001b[0;32m--> 433\u001b[0m     layer_output \u001b[39m=\u001b[39m albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n\u001b[1;32m    434\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/albert/modeling_albert.py:396\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m     output_hidden_states: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    395\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 396\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden_states, attention_mask, head_mask, output_attentions)\n\u001b[1;32m    398\u001b[0m     ffn_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    399\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff_chunk,\n\u001b[1;32m    400\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward,\n\u001b[1;32m    401\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim,\n\u001b[1;32m    402\u001b[0m         attention_output[\u001b[39m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_layer_layer_norm(ffn_output \u001b[39m+\u001b[39m attention_output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/albert/modeling_albert.py:322\u001b[0m, in \u001b[0;36mAlbertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    321\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor], Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> 322\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    323\u001b[0m     mixed_key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states)\n\u001b[1;32m    324\u001b[0m     mixed_value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For each class in class_code, generate embeddings and add to class_embeddings dictionary\n",
    "class_embeddings = {}\n",
    "class_code = load_class_code_from_directory(system)\n",
    "for class_name, code in class_code.items():\n",
    "    class_embeddings[class_name] = generate_embeddings_for_java_file(code, model, tokenizer, device)\n",
    "\n",
    "# Write embeddings to csv file\n",
    "save_embeddings_to_csv(version, system, model_type, class_embeddings, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{version}_{system}_{model_type}_embeddings.csv\"\n",
    "class_names, labels, embeddings = load_data_from_csv(filename)\n",
    "\n",
    "Xtrain, Xtest, names_train, names_test = train_test_split(embeddings, class_names, test_size=0.3, random_state=0)\n",
    "\n",
    "ytrain = [labels[class_names.index(name)] for name in names_train]\n",
    "ytest = [labels[class_names.index(name)] for name in names_test]\n",
    "\n",
    "Xtrain = np.array(Xtrain)\n",
    "Xtest = np.array(Xtest)\n",
    "\n",
    "# Ensure that there's at least one instance of the \"Utility\" label in the training data\n",
    "if 1 not in ytrain:\n",
    "    utility_index = labels.index(1)\n",
    "    Xtrain = np.append(Xtrain, [embeddings[utility_index]], axis=0)\n",
    "    ytrain.append(1)\n",
    "\n",
    "print(Xtrain)\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(y_true, y_pred):\n",
    "    # Identify unique labels in both true labels and predictions\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    dynamic_label_names = [label_names_map[label] for label in unique_labels]\n",
    "\n",
    "    # Generate and print the classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=dynamic_label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=2).fit(Xtrain, ytrain)\n",
    "decision_tree_predictions = decision_tree_classifier.predict(Xtest)\n",
    "decision_tree_accuracy = accuracy_score(ytest, decision_tree_predictions)\n",
    "decision_tree_confusion_matrix = confusion_matrix(ytest, decision_tree_predictions)\n",
    "print(decision_tree_accuracy)\n",
    "print(decision_tree_confusion_matrix)\n",
    "generate_classification_report(ytest, decision_tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='linear', C=2).fit(Xtrain, ytrain)\n",
    "svm_predictions = svm_classifier.predict(Xtest)\n",
    "svm_accuracy = accuracy_score(ytest, svm_predictions)\n",
    "svm_confusion_matrix = confusion_matrix(ytest, svm_predictions)\n",
    "print(svm_accuracy)\n",
    "print(svm_confusion_matrix)\n",
    "generate_classification_report(ytest, svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)\n",
    "knn_predictions = knn_classifier.predict(Xtest)\n",
    "knn_accuracy = accuracy_score(ytest, knn_predictions)\n",
    "knn_confusion_matrix = confusion_matrix(ytest, knn_predictions)\n",
    "print(knn_accuracy)\n",
    "print(knn_confusion_matrix)\n",
    "generate_classification_report(ytest, knn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier = LogisticRegression(random_state=0).fit(Xtrain, ytrain)\n",
    "logistic_regression_predictions = logistic_regression_classifier.predict(Xtest)\n",
    "logistic_regression_accuracy = accuracy_score(ytest, logistic_regression_predictions)\n",
    "logistic_regression_confusion_matrix = confusion_matrix(ytest, logistic_regression_predictions)\n",
    "print(logistic_regression_accuracy)\n",
    "print(logistic_regression_confusion_matrix)\n",
    "generate_classification_report(ytest, logistic_regression_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = GaussianNB().fit(Xtrain, ytrain)\n",
    "naive_bayes_predictions = naive_bayes_classifier.predict(Xtest)\n",
    "naive_bayes_accuracy = accuracy_score(ytest, naive_bayes_predictions)\n",
    "naive_bayes_confusion_matrix = confusion_matrix(ytest, naive_bayes_predictions)\n",
    "print(naive_bayes_accuracy)\n",
    "print(naive_bayes_confusion_matrix)\n",
    "generate_classification_report(ytest, naive_bayes_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

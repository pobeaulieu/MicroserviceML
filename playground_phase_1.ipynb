{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Classification (assign Utility, Application, or Entity Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v_imen\" # All options: v_imen, v_team\n",
    "system = \"pos\" # All options: jforum, cargotracker, petclinic, pos\n",
    "model_type = \"bert\" # All options: ft_codebert, word2vec, albert, codebert, roberta, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AlbertTokenizer, AlbertModel, RobertaModel, RobertaTokenizer, BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils import load_class_code_from_directory, load_data_from_csv, write_embeddings_to_csv, associate_classes_to_types\n",
    "from embeddings import generate_embeddings_for_java_code, generate_word_embeddings_for_java_code\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU) is available and if so, set the device to GPU\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './codebert_finetuned'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/codebert-base\u001b[39m\u001b[39m\"\u001b[39m,force_download\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39melif\u001b[39;00m (model_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mft_codebert\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m./codebert_finetuned\u001b[39;49m\u001b[39m\"\u001b[39;49m,force_download\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m./codebert_finetuned\u001b[39m\u001b[39m\"\u001b[39m,force_download\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[39melif\u001b[39;00m (model_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:718\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    717\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    720\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:550\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m     token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m    549\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 550\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    551\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    552\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    553\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    554\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    555\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    556\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    557\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    558\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    559\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    560\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    561\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    562\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    563\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    564\u001b[0m )\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    431\u001b[0m         path_or_repo_id,\n\u001b[1;32m    432\u001b[0m         filename,\n\u001b[1;32m    433\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    434\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    435\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    436\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    437\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    438\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    439\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    440\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    441\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    442\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n\u001b[1;32m    171\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot have -- or .. in repo_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './codebert_finetuned'."
     ]
    }
   ],
   "source": [
    "# Select the model and tokenizer\n",
    "if (model_type == \"codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"microsoft/codebert-base\",force_download=False)\n",
    "elif (model_type == \"ft_codebert\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "    model = AutoModel.from_pretrained(\"./codebert_finetuned\",force_download=False)\n",
    "elif (model_type == \"bert\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "elif (model_type == \"roberta\"):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "elif (model_type == \"albert\"): \n",
    "    # pip3 install sentencepiece\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "    model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "elif model_type == \"word2vec\":\n",
    "    # Download required NLTK datasets and initialize the lemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Load Word2Vec model\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "else:\n",
    "    raise NameError(\"model type not supported\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are 0: Application, 1: Utility, 2: Entity\n",
    "class_labels = associate_classes_to_types(version, system)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class in class_code, generate embeddings and add to class_embeddings dictionary\n",
    "class_embeddings = {}\n",
    "class_code = load_class_code_from_directory(system)\n",
    "if model_type == \"word2vec\":\n",
    "    class_embeddings = {class_name: generate_word_embeddings_for_java_code(code, word2vec_model, word_lemmatizer) for class_name, code in class_code.items()}\n",
    "else:\n",
    "    class_embeddings = {class_name: generate_embeddings_for_java_code(code, model, tokenizer, device) for class_name, code in class_code.items()}\n",
    "\n",
    "# Write embeddings to csv file\n",
    "write_embeddings_to_csv(version, system, model_type, class_embeddings, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filename = f\"./generated_data/embedding/{version}_{system}_{model_type}_embeddings.csv\"\n",
    "class_names, labels, embeddings = load_data_from_csv(filename)\n",
    "\n",
    "# Train-test split\n",
    "Xtrain, Xtest, names_train, names_test = train_test_split(embeddings, class_names, test_size=0.3, random_state=0)\n",
    "\n",
    "# Get labels for the training and test sets\n",
    "ytrain, ytest = [[labels[class_names.index(name)] for name in lst] for lst in [names_train, names_test]]\n",
    "\n",
    "# Ensure at least one instance of each class in the training data\n",
    "unique_classes = set(labels)\n",
    "for cls in unique_classes:\n",
    "    if cls not in ytrain:\n",
    "        cls_index = labels.index(cls)\n",
    "        Xtrain = np.vstack([Xtrain, [embeddings[cls_index]]])\n",
    "        ytrain.append(cls)\n",
    "\n",
    "# Calculate class frequencies and mean frequency\n",
    "class_freq = Counter(ytrain)\n",
    "mean_count = sum(class_freq.values()) // len(class_freq)\n",
    "\n",
    "# Identify classes that need resampling (e.g., significantly fewer than mean_count)\n",
    "threshold = 0.7  # 70% of the mean_count\n",
    "classes_to_resample = {cls: int(mean_count) for cls, count in class_freq.items() if count < mean_count * threshold}\n",
    "\n",
    "# Check if all classes_to_resample have enough samples\n",
    "skip_resampling = False\n",
    "for class_label in classes_to_resample.keys():\n",
    "    if class_label in class_freq and class_freq[class_label] < 2:  # k_neighbors + 1\n",
    "        skip_resampling = True\n",
    "        print(f\"Skipping resampling for class {class_label} due to insufficient samples.\")\n",
    "        break\n",
    "\n",
    "# Apply SMOTE\n",
    "if not skip_resampling and classes_to_resample:\n",
    "    sm = SMOTE(sampling_strategy=classes_to_resample, k_neighbors=1, random_state=42)\n",
    "    Xtrain, ytrain = sm.fit_resample(Xtrain, ytrain)\n",
    "\n",
    "# Calculate total type repartition\n",
    "\n",
    "# Print information\n",
    "print(f'Number of classes: {len(class_names)}\\nResampled dataset shape: {Counter(ytrain)}')\n",
    "print(f'len(Xtrain): {len(Xtrain)}\\nlen(Xtest): {len(Xtest)}')\n",
    "print(f'ytrain: {ytrain}\\nytest: {ytest}')\n",
    "print(f'ytrain count: {np.bincount(ytrain)}\\nytest count: {np.bincount(ytest)}')\n",
    "print(f'total class types repartition : {np.bincount(ytrain) + np.bincount(ytest)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Please keep this code commented out unless you want to use it\n",
    "# This code ensures that each class is represented in the test set, proportional to its representation in the training set\n",
    "# Calculate class frequencies for training and test sets\n",
    "train_class_freq = Counter(ytrain)\n",
    "test_class_freq = Counter(ytest)\n",
    "\n",
    "# Calculate the ratio between the size of the training and test sets\n",
    "ratio = len(ytrain) / len(ytest)\n",
    "\n",
    "# Iterate over each class to make sure it has proportional representation in the test set\n",
    "for cls, train_count in train_class_freq.items():\n",
    "    expected_test_count = int(train_count / ratio)\n",
    "    actual_test_count = test_class_freq.get(cls, 0)\n",
    "\n",
    "    if actual_test_count < expected_test_count:\n",
    "        # Find instances in the training set to move to the test set\n",
    "        for _ in range(expected_test_count - actual_test_count):\n",
    "            cls_index = ytrain.index(cls)\n",
    "            Xtest = np.vstack([Xtest, [Xtrain[cls_index]]])\n",
    "            ytest.append(cls)\n",
    "            Xtrain = np.delete(Xtrain, cls_index, axis=0)\n",
    "            ytrain.pop(cls_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des rapports de classification (version textuelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(y_true, y_pred):\n",
    "    # Identify unique labels in both true labels and predictions\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    dynamic_label_names = [label_names_map[label] for label in unique_labels]\n",
    "\n",
    "    # Generate and print the classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=dynamic_label_names, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des fichiers CSV pour la classification de chaque modèle et modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_to_csv(y_true, y_pred, model_name, embedding_model):\n",
    "\n",
    "    csv_file = f'generated_data/visualization/classification_reports_{embedding_model}.csv'\n",
    "\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    dynamic_label_names = [label_names_map[label] for label in unique_labels]\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=dynamic_label_names, output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Create a DataFrame from the report\n",
    "    new_data = pd.DataFrame(report).transpose().reset_index()\n",
    "    new_data.columns = ['label', 'precision', 'recall', 'f1-score', 'support']\n",
    "    new_data['model_name'] = model_name\n",
    "    \n",
    "    # If CSV file exists and is non-empty, load it and filter out old model data\n",
    "    if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n",
    "        existing_data = pd.read_csv(csv_file)\n",
    "        # Filter out the old data for the current model\n",
    "        existing_data = existing_data[existing_data['model_name'] != model_name]\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Concatenate new data with existing data\n",
    "    combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to CSV\n",
    "    combined_data.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class types repartition representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_counts(ytrain_count, ytest_count):\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "    labels = [label_names_map[i] for i in range(len(ytrain_count))]\n",
    "\n",
    "    # Colors for each bar\n",
    "    colors_train = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "    colors_test = ['blue', 'green', 'red']\n",
    "\n",
    "    # Generate the histogram\n",
    "    plt.bar(labels, ytrain_count, color=colors_train, label='ytrain')\n",
    "    plt.bar(labels, ytest_count, bottom=ytrain_count, color=colors_test, label='ytest')\n",
    "    \n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Total type repartition')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_counts(np.bincount(ytrain), np.bincount(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération d'histogramme pour la répartition des élèments par classe par modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_for_types(y_true, y_pred, prediction_model_name):\n",
    "    # Map unique labels to their corresponding names\n",
    "    label_names_map = {-1: \"None\", 0: \"Application\", 1: \"Utility\", 2: \"Entity\"}\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
    "\n",
    "    # Extract support values\n",
    "    supports = [report[str(key)]['support'] for key in sorted(report.keys())[:-3]]\n",
    "    labels = [label_names_map[int(key)] for key in sorted(report.keys())[:-3]]\n",
    "\n",
    "    # Colors for each bar\n",
    "    colors = ['blue', 'green', 'red']\n",
    "\n",
    "    # Generate the histogram\n",
    "    plt.bar(labels, supports, color=colors)\n",
    "    plt.ylabel('Number of classes')\n",
    "    plt.title(f'Type repartition using {prediction_model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=2).fit(Xtrain, ytrain)\n",
    "decision_tree_predictions = decision_tree_classifier.predict(Xtest)\n",
    "decision_tree_accuracy = accuracy_score(ytest, decision_tree_predictions)\n",
    "decision_tree_confusion_matrix = confusion_matrix(ytest, decision_tree_predictions)\n",
    "print(decision_tree_accuracy)\n",
    "print(decision_tree_confusion_matrix)\n",
    "generate_classification_report(ytest, decision_tree_predictions)\n",
    "generate_classification_report_for_types(ytest, decision_tree_predictions, \"Decision Tree\")\n",
    "generate_classification_report_to_csv(ytest, decision_tree_predictions, \"decision_tree\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='linear', C=2, probability=True).fit(Xtrain, ytrain)\n",
    "svm_predictions = svm_classifier.predict(Xtest)\n",
    "svm_accuracy = accuracy_score(ytest, svm_predictions)\n",
    "svm_confusion_matrix = confusion_matrix(ytest, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(svm_confusion_matrix)\n",
    "generate_classification_report(ytest, svm_predictions)\n",
    "generate_classification_report_for_types(ytest, svm_predictions, \"SVM\")\n",
    "generate_classification_report_to_csv(ytest, svm_predictions, \"svm\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)\n",
    "knn_predictions = knn_classifier.predict(Xtest)\n",
    "knn_accuracy = accuracy_score(ytest, knn_predictions)\n",
    "knn_confusion_matrix = confusion_matrix(ytest, knn_predictions)\n",
    "print(knn_accuracy)\n",
    "print(knn_confusion_matrix)\n",
    "generate_classification_report(ytest, knn_predictions)\n",
    "generate_classification_report_for_types(ytest, knn_predictions, \"KNN\")\n",
    "generate_classification_report_to_csv(ytest, knn_predictions, \"knn\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier = LogisticRegression(random_state=0).fit(Xtrain, ytrain)\n",
    "logistic_regression_predictions = logistic_regression_classifier.predict(Xtest)\n",
    "logistic_regression_accuracy = accuracy_score(ytest, logistic_regression_predictions)\n",
    "logistic_regression_confusion_matrix = confusion_matrix(ytest, logistic_regression_predictions)\n",
    "print(logistic_regression_accuracy)\n",
    "print(logistic_regression_confusion_matrix)\n",
    "generate_classification_report(ytest, logistic_regression_predictions)\n",
    "generate_classification_report_for_types(ytest, logistic_regression_predictions, \"Logistic Regression\")\n",
    "generate_classification_report_to_csv(ytest, logistic_regression_predictions, \"logistic_regression\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = GaussianNB().fit(Xtrain, ytrain)\n",
    "naive_bayes_predictions = naive_bayes_classifier.predict(Xtest)\n",
    "naive_bayes_accuracy = accuracy_score(ytest, naive_bayes_predictions)\n",
    "naive_bayes_confusion_matrix = confusion_matrix(ytest, naive_bayes_predictions)\n",
    "print(naive_bayes_accuracy)\n",
    "print(naive_bayes_confusion_matrix)\n",
    "generate_classification_report(ytest, naive_bayes_predictions)\n",
    "generate_classification_report_for_types(ytest, naive_bayes_predictions, \"Gaussian NB\")\n",
    "generate_classification_report_to_csv(ytest, naive_bayes_predictions, \"gaussian_nb\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual classifiers into an ensemble\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "('svm', svm_classifier), ('knn', knn_classifier), ('dt', decision_tree_classifier), ('log_reg', logistic_regression_classifier), ('gnb', naive_bayes_classifier)],\n",
    "voting='soft')\n",
    "\n",
    "# Use SVM as the base estimator for AdaBoost\n",
    "svm_base = SVC(kernel='linear', C=2, probability=True) # Can use any other classifier as the base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=svm_base, n_estimators=50, algorithm='SAMME.R', random_state=1)\n",
    "\n",
    "# Combine the ensemble classifier with AdaBoost\n",
    "final_ensemble = VotingClassifier(estimators=[\n",
    "    ('ensemble_clf', ensemble_clf), ('ada_boost', ada_boost)],\n",
    "    voting='soft')\n",
    "\n",
    "# Fit model to your data\n",
    "final_ensemble.fit(Xtrain, ytrain)\n",
    "\n",
    "# Evaluate model\n",
    "ensemble_predictions = final_ensemble.predict(Xtest)\n",
    "ensemble_accuracy = accuracy_score(ytest, ensemble_predictions)\n",
    "print('Accuracy:', ensemble_accuracy)\n",
    "ensemble_confusion_matrix = confusion_matrix(ytest, ensemble_predictions)\n",
    "print(ensemble_confusion_matrix)\n",
    "generate_classification_report(ytest, ensemble_predictions)\n",
    "generate_classification_report_for_types(ytest, ensemble_predictions, \"Ensemble learning\")\n",
    "generate_classification_report_to_csv(ytest, ensemble_predictions, \"ensemble_learning\", model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
